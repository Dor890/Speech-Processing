{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1hzElIYzQPndRw0h9IfyNYi_UcmVLPQ-c",
      "authorship_tag": "ABX9TyMaa/gUZeOYpyF5622Fwv0/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dor890/Speech-Processing/blob/master/ASR_fin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jiwer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKeW974s6HGG",
        "outputId": "c4e98146-a7d4-46c2-90f0-3f71dfc1e830"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jiwer\n",
            "  Downloading jiwer-3.0.2-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (8.1.6)\n",
            "Collecting rapidfuzz==2.13.7 (from jiwer)\n",
            "  Downloading rapidfuzz-2.13.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
            "Successfully installed jiwer-3.0.2 rapidfuzz-2.13.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchaudio.models.decoder import ctc_decoder\n",
        "from torchaudio.models.decoder._ctc_decoder import download_pretrained_files\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "from jiwer import wer, cer\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "files = download_pretrained_files(\"librispeech-4-gram\")\n"
      ],
      "metadata": {
        "id": "wjFoB3526LH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Numerical constants\n",
        "SR = 16000\n",
        "FILE_2CHECK = 13\n",
        "HOP_LEN = 160\n",
        "N_FFT = 400\n",
        "N_MELS = 128  # 128 for Mel_Spec, 23 for MFCC\n",
        "N_MFCC = 13\n",
        "N_EPOCHS = 1000\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.0004\n",
        "WEIGHT_DECAY = 0.001\n",
        "NUM_LAYERS = 12\n",
        "HIDDEN_DIM = 64\n",
        "EMBED_DIM = 300\n",
        "NUM_CLASSES = 29\n",
        "TIME = 513  # 513 for Mel_Spec, 641 for MFCC\n",
        "PAD_TOKEN = 0\n",
        "SEQ_LEN = 3\n",
        "DROPOUT = 0.1\n",
        "\n",
        "# Strings constants\n",
        "CTC_MODEL_PATH = 'models/ctc_model.pth'\n",
        "LANG_MODEL_PATH = 'models/lang_model.pth'\n",
        "DATA_PATH = '/content/drive/MyDrive/an4'\n",
        "\n",
        "hparams = {\n",
        "    \"n_cnn_layers\": 3,\n",
        "    \"n_rnn_layers\": 5,\n",
        "    \"rnn_dim\": 512,\n",
        "    \"n_class\": 29,\n",
        "    \"n_feats\": 128,\n",
        "    \"stride\": 2,\n",
        "    \"dropout\": DROPOUT,\n",
        "    \"learning_rate\": LEARNING_RATE,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"epochs\": N_EPOCHS\n",
        "}"
      ],
      "metadata": {
        "id": "r_nRg0LDLT0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TextTransform:\n",
        "    def __init__(self):\n",
        "        char_map_str = \"\"\"\n",
        "            ' 0\n",
        "            SPACE 1\n",
        "            a 2\n",
        "            b 3\n",
        "            c 4\n",
        "            d 5\n",
        "            e 6\n",
        "            f 7\n",
        "            g 8\n",
        "            h 9\n",
        "            i 10\n",
        "            j 11\n",
        "            k 12\n",
        "            l 13\n",
        "            m 14\n",
        "            n 15\n",
        "            o 16\n",
        "            p 17\n",
        "            q 18\n",
        "            r 19\n",
        "            s 20\n",
        "            t 21\n",
        "            u 22\n",
        "            v 23\n",
        "            w 24\n",
        "            x 25\n",
        "            y 26\n",
        "            z 27\n",
        "            \"\"\"\n",
        "        self.char_map = {}\n",
        "        self.index_map = {}\n",
        "        for line in char_map_str.strip().split('\\n'):\n",
        "            ch, index = line.split()\n",
        "            self.char_map[ch] = int(index)\n",
        "            self.index_map[int(index)] = ch\n",
        "        self.index_map[1] = ' '\n",
        "\n",
        "    def text_to_int(self, text):\n",
        "        int_sequence = []\n",
        "        for c in text:\n",
        "            if c == ' ':\n",
        "                ch = self.char_map['SPACE']\n",
        "            else:\n",
        "                ch = self.char_map[c]\n",
        "            int_sequence.append(ch)\n",
        "        return int_sequence\n",
        "\n",
        "    def int_to_text(self, labels):\n",
        "        string = []\n",
        "        for i in labels:\n",
        "            string.append(self.index_map[i])\n",
        "        return ''.join(string).replace('SPACE', ' ')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Lo_obzPXLb8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Decoders\n",
        "def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n",
        "    arg_maxes = torch.argmax(output, dim=2)\n",
        "    decodes = []\n",
        "    targets = []\n",
        "    text_transform = TextTransform()\n",
        "    for i, args in enumerate(arg_maxes):\n",
        "        decode = []\n",
        "        targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
        "        for j, index in enumerate(args):\n",
        "            if index != blank_label:\n",
        "                if collapse_repeated and j != 0 and index == args[j -1]:\n",
        "                    continue\n",
        "                decode.append(index.item())\n",
        "        decodes.append(text_transform.int_to_text(decode))\n",
        "    return decodes, targets\n",
        "\n"
      ],
      "metadata": {
        "id": "UNpfsbTt6Oi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2ep0TP9KzRH"
      },
      "outputs": [],
      "source": [
        "class AN4Dataset(Dataset):\n",
        "    def __init__(self, split, transform=None, target_transform=None):\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "        audio_dir = os.path.join(DATA_PATH, split, 'an4', 'wav')\n",
        "        transcript_dir = os.path.join(DATA_PATH, split, 'an4', 'txt')\n",
        "\n",
        "        audio_files = sorted(os.listdir(audio_dir))\n",
        "        transcript_files = sorted(os.listdir(transcript_dir))\n",
        "\n",
        "        audio_paths = [os.path.join(audio_dir, file) for file in audio_files if file.endswith('wav')]\n",
        "        transcript_paths = [os.path.join(transcript_dir, file) for file in\n",
        "                            transcript_files if file.endswith('txt')]\n",
        "\n",
        "        self.audios, self.transcripts = [], []\n",
        "\n",
        "        for audio_path, transcript_path in zip(audio_paths, transcript_paths):\n",
        "            with open(transcript_path, 'r') as f:\n",
        "                transcript = f.read().strip()\n",
        "                self.transcripts.append(transcript)\n",
        "\n",
        "        self.loaded_audios = [torchaudio.load(audio)[0] for audio in audio_paths]\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.loaded_audios)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.loaded_audios[idx], self.transcripts[idx]\n",
        "\n",
        "\n",
        "def data_processing(data, data_type=\"train\"):\n",
        "    inputs, inputs_lengths, labels, labels_length = [], [], [], []\n",
        "\n",
        "    if data_type == \"train\":\n",
        "        transform = nn.Sequential(\n",
        "            torchaudio.transforms.MelSpectrogram(sample_rate=SR, n_mels=N_MELS),\n",
        "            torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
        "            torchaudio.transforms.TimeMasking(time_mask_param=100))\n",
        "    else:\n",
        "        transform = torchaudio.transforms.MelSpectrogram()\n",
        "\n",
        "    text_transform = TextTransform()\n",
        "    for (wav, transcript) in data:\n",
        "        spec = transform(wav).squeeze(0).transpose(0, 1)\n",
        "        inputs.append(spec)\n",
        "        inputs_lengths.append(spec.shape[0] // 2)\n",
        "        label = torch.Tensor(text_transform.text_to_int(str(transcript).lower()))\n",
        "        labels.append(label)\n",
        "        labels_length.append(len(label))\n",
        "\n",
        "    max_length = (max(inputs_lengths) * 2) + 1\n",
        "\n",
        "    # Pad tensors and create the big tensor\n",
        "    spectrograms = torch.zeros((len(inputs), max_length, N_MELS))\n",
        "    for i, tensor in enumerate(inputs):\n",
        "        spectrograms[i, :tensor.shape[0], :] = tensor[:, :]\n",
        "\n",
        "    spectrograms = spectrograms.unsqueeze(1).transpose(2, 3)\n",
        "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "    return spectrograms, labels, inputs_lengths, labels_length\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNLayerNorm(nn.Module):\n",
        "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
        "\n",
        "    def __init__(self, n_feats):\n",
        "        super(CNNLayerNorm, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x (batch, channel, feature, time)\n",
        "        x = x.transpose(2, 3).contiguous()  # (batch, channel, time, feature)\n",
        "        x = self.layer_norm(x)\n",
        "        return x.transpose(2, 3).contiguous()  # (batch, channel, feature, time)\n",
        "\n",
        "\n",
        "class ResidualCNN(nn.Module):\n",
        "    \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n",
        "        except with layer norm instead of batch norm\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel, stride, dropout,\n",
        "                 n_feats):\n",
        "        super(ResidualCNN, self).__init__()\n",
        "\n",
        "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride,\n",
        "                              padding=kernel // 2)\n",
        "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride,\n",
        "                              padding=kernel // 2)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
        "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x  # (batch, channel, feature, time)\n",
        "        x = self.layer_norm1(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.cnn1(x)\n",
        "        x = self.layer_norm2(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.cnn2(x)\n",
        "        x += residual\n",
        "        return x  # (batch, channel, feature, time)\n",
        "\n",
        "\n",
        "class BidirectionalGRU(nn.Module):\n",
        "\n",
        "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
        "        super(BidirectionalGRU, self).__init__()\n",
        "\n",
        "        self.BiGRU = nn.GRU(\n",
        "            input_size=rnn_dim, hidden_size=hidden_size,\n",
        "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
        "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        x = F.gelu(x)\n",
        "        x, _ = self.BiGRU(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SpeechRecognitionModel(nn.Module):\n",
        "    \"\"\"Speech Recognition Model Inspired by DeepSpeech 2\"\"\"\n",
        "\n",
        "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim,\n",
        "                 n_class, n_feats,\n",
        "                 stride=2, dropout=0.1):\n",
        "        super(SpeechRecognitionModel, self).__init__()\n",
        "        # self.vocabulary = vocabulary\n",
        "        n_feats = n_feats // 2\n",
        "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride,\n",
        "                             padding=3 // 2)  # cnn for extracting heirachal features\n",
        "\n",
        "        # n residual cnn layers with filter size of 32\n",
        "        self.rescnn_layers = nn.Sequential(*[\n",
        "            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout,\n",
        "                        n_feats=n_feats)\n",
        "            for _ in range(n_cnn_layers)\n",
        "        ])\n",
        "        self.fully_connected = nn.Linear(n_feats * 32, rnn_dim)\n",
        "        self.birnn_layers = nn.Sequential(*[\n",
        "            BidirectionalGRU(rnn_dim=rnn_dim if i == 0 else rnn_dim * 2,\n",
        "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i == 0)\n",
        "            for i in range(n_rnn_layers)\n",
        "        ])\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(rnn_dim * 2, rnn_dim),  # birnn returns rnn_dim*2\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(rnn_dim, n_class)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x)\n",
        "        x = self.rescnn_layers(x)\n",
        "        sizes = x.size()\n",
        "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
        "        x = x.transpose(1, 2)  # (batch, time, feature)\n",
        "        x = self.fully_connected(x)\n",
        "        x = self.birnn_layers(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "ceCaO2YleiRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    \"\"\"\n",
        "    A basic LSTM models for speech recognition.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocabulary, lang_model=None):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.vocabulary = vocabulary\n",
        "        self.lang_model = lang_model\n",
        "\n",
        "        # RNN layers\n",
        "        self.rnn = nn.LSTM(input_size=4032, hidden_size=HIDDEN_DIM,\n",
        "                           num_layers=NUM_LAYERS, batch_first=True)\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(HIDDEN_DIM, NUM_CLASSES)\n",
        "\n",
        "        # Decoders\n",
        "        # self.greedy_decoder = GreedyDecoder(vocabulary.translator.values())\n",
        "        # self.beam_decoder = ctc_decoder(lexicon='lexicon.txt',\n",
        "        #                                 tokens='tokens.txt', lm=files.lm)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        sizes = x.size()\n",
        "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
        "        x = x.transpose(1, 2)  # (batch, time, feature)\n",
        "        rnn_output, _ = self.rnn(x)\n",
        "        output = self.fc(rnn_output)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "mmBxwBRB5ZBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "files = download_pretrained_files(\"librispeech-4-gram\")\n",
        "\n",
        "def save_model(model, path):\n",
        "    \"\"\"\n",
        "    Saves a pytorch models to the given path.\n",
        "    \"\"\"\n",
        "    torch.save(model.state_dict(), '{}'.format(path))\n",
        "\n",
        "\n",
        "def load_model(model, path):\n",
        "    \"\"\"\n",
        "    Loads a pytorch models from the given path. The models should already by\n",
        "    created (e.g. by calling the constructor) and should be passed as an argument.\n",
        "    \"\"\"\n",
        "    model.load_state_dict(torch.load('{}'.format(path)))\n",
        "\n",
        "\n",
        "\n",
        "def test(model, test_loader, criterion, experiment):\n",
        "    print('evaluating...')\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_cer, test_wer = [], []\n",
        "    with experiment.test():\n",
        "        with torch.no_grad():\n",
        "            for i, _data in enumerate(test_loader):\n",
        "                spectrograms, labels, input_lengths, label_lengths = _data\n",
        "                spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "                output = model(spectrograms)  # (batch, time, n_class)\n",
        "                output = F.log_softmax(output, dim=2)\n",
        "                output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "                loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "                test_loss += loss.item() / len(test_loader)\n",
        "\n",
        "                decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
        "                for j in range(len(decoded_preds)):\n",
        "                    test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
        "                    test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
        "\n",
        "\n",
        "\n",
        "    avg_cer = sum(test_cer)/len(test_cer)\n",
        "    avg_wer = sum(test_wer)/len(test_wer)\n",
        "    experiment.log_metric('test_loss', test_loss)\n",
        "    experiment.log_metric('cer', avg_cer)\n",
        "    experiment.log_metric('wer', avg_wer)\n",
        "\n",
        "    print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))\n",
        "\n",
        "\n",
        "def train(model, train_loader, criterion, experiment):\n",
        "    data_len = len(train_loader.dataset)\n",
        "    optimizer = torch.optim.RMSprop(model.parameters(), LEARNING_RATE)\n",
        "    model.train()\n",
        "    model = model.to(device)\n",
        "    with experiment.train():\n",
        "      counter = 0\n",
        "      for epoch in range(N_EPOCHS):\n",
        "          if (epoch + 1) % 5 == 0:\n",
        "              save_model(model, CTC_MODEL_PATH)\n",
        "          e_loss = 0\n",
        "          for batch_idx, _data in enumerate(train_loader):\n",
        "              spectrograms, labels, input_lengths, label_lengths = _data\n",
        "              spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "              optimizer.zero_grad()\n",
        "              output = model(spectrograms)  # (batch, time, n_class)\n",
        "              output = F.log_softmax(output, dim=2)\n",
        "              output = output.transpose(0, 1)  # (time, batch, n_class)\n",
        "\n",
        "              loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "              loss.backward()\n",
        "\n",
        "              # log it\n",
        "              experiment.log_metric('loss', loss.item(), step=counter)\n",
        "\n",
        "              optimizer.step()\n",
        "              counter += 1\n",
        "              e_loss += loss.item()\n",
        "          print(f\"Train Epoch: {epoch}, loss = {e_loss}\")\n",
        "\n",
        "    save_model(model, CTC_MODEL_PATH)\n"
      ],
      "metadata": {
        "id": "8olxeA0FK4SA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ed857da-4fa9-429e-c39d-61b2ba1bcf15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchaudio/models/decoder/_ctc_decoder.py:62: UserWarning: The built-in flashlight integration is deprecated, and will be removed in future release. Please install flashlight-text. https://pypi.org/project/flashlight-text/ For the detail of CTC decoder migration, please see https://github.com/pytorch/audio/issues/3088.\n",
            "  warnings.warn(\n",
            "100%|██████████| 4.97M/4.97M [00:00<00:00, 36.4MB/s]\n",
            "100%|██████████| 57.0/57.0 [00:00<00:00, 79.3kB/s]\n",
            "100%|██████████| 2.91G/2.91G [00:43<00:00, 72.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# setp up logging platform\n",
        "\n",
        "comet_api_key = \"P9a9lCzL9VwojP7DYNjYZLrdl\" # add your api key here\n",
        "project_name = \"67455 Introduction to Speech Processing - Final project\"\n",
        "experiment_name = \"weak net no pre-train\"\n",
        "\n",
        "if comet_api_key:\n",
        "  experiment = Experiment(api_key=comet_api_key, project_name=project_name, parse_args=False)\n",
        "  experiment.set_name(experiment_name)\n",
        "  experiment.display()\n",
        "else:\n",
        "  experiment = Experiment(api_key='dummy_key', disabled=True)"
      ],
      "metadata": {
        "id": "8iptwPXo2QAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_alignments(waveform, emission, tokens, timesteps):\n",
        "    \"\"\"\n",
        "    Plots the alignment between the waveform and the predicted transcription.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(32, 10))\n",
        "    ax.plot(waveform)\n",
        "\n",
        "    ratio = waveform.shape[0] / emission.shape[1]\n",
        "    word_start = 0\n",
        "    for i in range(len(tokens)):\n",
        "        if i != 0 and tokens[i - 1] == \"|\":\n",
        "            word_start = timesteps[i]\n",
        "        if tokens[i] != \"|\":\n",
        "            plt.annotate(tokens[i].upper(), (timesteps[i] * ratio, waveform.max() * 1.02), size=14)\n",
        "        elif i != 0:\n",
        "            word_end = timesteps[i]\n",
        "            ax.axvspan(word_start * ratio, word_end * ratio, alpha=0.1, color=\"red\")\n",
        "\n",
        "    xticks = ax.get_xticks()\n",
        "    plt.xticks(xticks, xticks / SR)\n",
        "    ax.set_xlabel(\"Time\")\n",
        "    ax.set_xlim(0, waveform.shape[0])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def test_distance_algorithms(data):\n",
        "    \"\"\"\n",
        "    Test the Distances algorithms (DTW & Euclidean) as the most naive\n",
        "    implementations.\n",
        "    \"\"\"\n",
        "    x_train, y_train = data.get_data('train')\n",
        "    x_val, y_val = data.get_data('val')\n",
        "    x_test, y_test = data.get_data('test')\n",
        "\n",
        "    dtw = DTWModel(x_train, y_train)\n",
        "    dtw.add_data(x_val, y_val)\n",
        "    predictions_dtw = dtw.classify_using_DTW_distance(x_test)\n",
        "    print('Predictions:')\n",
        "    print(predictions_dtw[:5])\n",
        "    print('True labels:')\n",
        "    print(y_test[:5])\n",
        "    print('Testing DTW algorithm...')\n",
        "    wer_error = wer(y_test, predictions_dtw)\n",
        "    cer_error = cer(y_test, predictions_dtw)\n",
        "    print(f'DTW Test WER: {wer_error:.4f}')\n",
        "    print(f'DTW Test CER: {cer_error:.4f}')\n",
        "    print('DTW tested successfully')\n",
        "\n",
        "    # print('Testing Euclidean algorithm...')\n",
        "    # euclidean = EuclideanModel(x_train, y_train)\n",
        "    # euclidean.add_data(x_val, y_val)\n",
        "    # predictions_euclidean = euclidean.classify_using_euclidean_distance(x_test)\n",
        "    # wer_error = wer(y_test, predictions_euclidean)\n",
        "    # cer_error = cer(y_test, predictions_euclidean)\n",
        "    # print('Predictions:')\n",
        "    # print(predictions_euclidean[:5])\n",
        "    # print('True labels:')\n",
        "    # print(y_test[:5])\n",
        "    # print(f'Euclidean Test WER: {wer_error:.4f}')\n",
        "    # print(f'Euclidean Test CER: {cer_error:.4f}')\n",
        "    # print('Euclidean tested successfully')\n",
        "\n",
        "\n",
        "def main():\n",
        "    print('--- Start running ---')\n",
        "    # test_distance_algorithms(data)\n",
        "\n",
        "    train_data_set = AN4Dataset('train')\n",
        "    test_data_set = AN4Dataset('test')\n",
        "    train_loader = DataLoader(dataset=train_data_set,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              shuffle=True,\n",
        "                              collate_fn=lambda x: data_processing(x, 'train'))\n",
        "    test_loader = DataLoader(dataset=test_data_set,\n",
        "                             batch_size=BATCH_SIZE,\n",
        "                             shuffle=False,\n",
        "                             collate_fn=lambda x: data_processing(x, 'val'))\n",
        "\n",
        "    # lang_model = language_model.LanguageModel(vocabulary)\n",
        "    print('Training the language models...')\n",
        "    # language_model.train_all_data(lang_model, y_train+y_val)\n",
        "    print('Language models trained successfully')\n",
        "    ctc_lstm = SpeechRecognitionModel(hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
        "                                            hparams['n_class'], hparams['n_feats'], hparams['stride'],\n",
        "                                                hparams['dropout']\n",
        "                                              ).to(device)\n",
        "\n",
        "   # ctc_lstm = LSTMModel(0).to(device)\n",
        "    lossFunc = nn.CTCLoss(blank=28, zero_infinity=True).to(device)\n",
        "    if os.path.exists(CTC_MODEL_PATH):\n",
        "        print('Loading the models...')\n",
        "        load_model(ctc_lstm, CTC_MODEL_PATH)\n",
        "        print('Model loaded successfully')\n",
        "    else:  # Train the models\n",
        "        print('Training the models...')\n",
        "        train_all_data(ctc_lstm, train_loader, lossFunc)\n",
        "    print('Model trained successfully')\n",
        "\n",
        "    test(ctc_lstm, test_loader, lossFunc)\n",
        "\n",
        "    # Evaluate the models on the test set\n",
        "    # print('Evaluating the models...')\n",
        "    # x_test, y_test = data.get_data('train')\n",
        "    # test_wer, test_cer = evaluate(ctc_lstm, x_test, y_test)\n",
        "    # print(f'Test WER: {test_wer:.4f}')\n",
        "    # print(f'Test CER: {test_cer:.4f}')\n",
        "    # print('Model evaluated successfully')\n",
        "    print('-- Finished running ---')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "-vj7zzNxK-h-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}