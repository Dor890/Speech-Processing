{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dor890/Speech-Processing/blob/master/FinalPorject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jiwer torch torchaudio comet_ml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKeW974s6HGG",
        "outputId": "b723aef9-7dd7-4151-c2d7-88390ce87c66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jiwer\n",
            "  Downloading jiwer-3.0.2-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.0.2+cu118)\n",
            "Collecting comet_ml\n",
            "  Downloading comet_ml-3.33.7-py3-none-any.whl (557 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.0/557.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (8.1.6)\n",
            "Collecting rapidfuzz==2.13.7 (from jiwer)\n",
            "  Downloading rapidfuzz-2.13.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (4.3.3)\n",
            "Requirement already satisfied: psutil>=5.6.3 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (5.9.5)\n",
            "Collecting python-box<7.0.0 (from comet_ml)\n",
            "  Downloading python_box-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-toolbelt>=0.8.0 (from comet_ml)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (2.27.1)\n",
            "Collecting semantic-version>=2.8.0 (from comet_ml)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting sentry-sdk>=1.1.0 (from comet_ml)\n",
            "  Downloading sentry_sdk-1.29.2-py2.py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.6/215.6 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting simplejson (from comet_ml)\n",
            "  Downloading simplejson-3.19.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from comet_ml) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (1.26.16)\n",
            "Collecting websocket-client<1.4.0,>=0.55.0 (from comet_ml)\n",
            "  Downloading websocket_client-1.3.3-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (1.14.1)\n",
            "Collecting wurlitzer>=1.0.2 (from comet_ml)\n",
            "  Downloading wurlitzer-3.0.3-py3-none-any.whl (7.3 kB)\n",
            "Collecting everett[ini]<3.2.0,>=1.0.1 (from comet_ml)\n",
            "  Downloading everett-3.1.0-py2.py3-none-any.whl (35 kB)\n",
            "Collecting dulwich!=0.20.33,>=0.20.6 (from comet_ml)\n",
            "  Downloading dulwich-0.21.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.1/510.1 kB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rich>=13.3.2 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (13.4.2)\n",
            "Collecting configobj (from everett[ini]<3.2.0,>=1.0.1->comet_ml)\n",
            "  Downloading configobj-5.0.8-py2.py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (23.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.19.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet_ml) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet_ml) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet_ml) (3.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.3.2->comet_ml) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.3.2->comet_ml) (2.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.3.2->comet_ml) (0.1.2)\n",
            "Installing collected packages: everett, wurlitzer, websocket-client, simplejson, sentry-sdk, semantic-version, rapidfuzz, python-box, dulwich, configobj, requests-toolbelt, jiwer, comet_ml\n",
            "  Attempting uninstall: websocket-client\n",
            "    Found existing installation: websocket-client 1.6.1\n",
            "    Uninstalling websocket-client-1.6.1:\n",
            "      Successfully uninstalled websocket-client-1.6.1\n",
            "Successfully installed comet_ml-3.33.7 configobj-5.0.8 dulwich-0.21.5 everett-3.1.0 jiwer-3.0.2 python-box-6.1.0 rapidfuzz-2.13.7 requests-toolbelt-1.0.0 semantic-version-2.10.0 sentry-sdk-1.29.2 simplejson-3.19.1 websocket-client-1.3.3 wurlitzer-3.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchaudio.models.decoder import ctc_decoder\n",
        "from torchaudio.models.decoder._ctc_decoder import download_pretrained_files\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "from jiwer import wer, cer\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from comet_ml import Experiment\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "files = download_pretrained_files(\"librispeech-4-gram\")\n"
      ],
      "metadata": {
        "id": "wjFoB3526LH-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5ccbedf-00a7-4aed-aaab-3dbdec8bbc26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchaudio/models/decoder/_ctc_decoder.py:62: UserWarning: The built-in flashlight integration is deprecated, and will be removed in future release. Please install flashlight-text. https://pypi.org/project/flashlight-text/ For the detail of CTC decoder migration, please see https://github.com/pytorch/audio/issues/3088.\n",
            "  warnings.warn(\n",
            "100%|██████████| 4.97M/4.97M [00:01<00:00, 3.54MB/s]\n",
            "100%|██████████| 57.0/57.0 [00:00<00:00, 165kB/s]\n",
            "100%|██████████| 2.91G/2.91G [00:43<00:00, 71.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "ikPqDXlNckS6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00714e95-1a81-44fc-e283-24e465218a7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Numerical constants\n",
        "SR = 16000\n",
        "FILE_2CHECK = 13\n",
        "HOP_LEN = 160\n",
        "N_FFT = 400\n",
        "N_MELS = 128\n",
        "N_MFCC = 13\n",
        "N_EPOCHS = 50\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 5e-4\n",
        "WEIGHT_DECAY = 0.001\n",
        "N_CNN_LAYERS = 3\n",
        "N_RNN_LAYERS = 5\n",
        "RNN_DIM = 512\n",
        "STRIDE = 2\n",
        "HIDDEN_DIM = 256\n",
        "EMBED_DIM = 300\n",
        "NUM_CLASSES = 29\n",
        "DROPOUT = 0.1\n",
        "\n",
        "# Strings constants\n",
        "CTC_MODEL_PATH = '/content/drive/MyDrive/models/ctc_model.pth'\n",
        "DATA_PATH = '/content/drive/MyDrive/an4'\n",
        "\n",
        "LEXICON_PATH = '/content/drive/MyDrive/models/lexicon.txt'\n",
        "TOKENS_PATH = '/content/drive/MyDrive/models/tokens.txt'\n"
      ],
      "metadata": {
        "id": "r_nRg0LDLT0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Data:\n",
        "    def __init__(self):\n",
        "        print('Loading data...')\n",
        "        self.data_dir = 'an4'\n",
        "        self.x_train_paths, self.x_train, self.y_train = self.load_data('train')\n",
        "        self.x_val_paths, self.x_val, self.y_val = self.load_data('val')\n",
        "        self.x_test_paths, self.x_test, self.y_test = self.load_data('test')\n",
        "        print('Data loaded successfully')\n",
        "\n",
        "    def load_data(self, split):\n",
        "        \"\"\"\n",
        "        Load the data from the provided 'an4' folder, and split it into train, dev, and test sets.\n",
        "        \"\"\"\n",
        "        audio_dir = os.path.join(self.data_dir, split, 'an4', 'wav')\n",
        "        transcript_dir = os.path.join(self.data_dir, split, 'an4', 'txt')\n",
        "\n",
        "        audio_files = sorted(os.listdir(audio_dir))\n",
        "        transcript_files = sorted(os.listdir(transcript_dir))\n",
        "\n",
        "        audio_paths = [os.path.join(audio_dir, file) for file in audio_files]\n",
        "        transcript_paths = [os.path.join(transcript_dir, file) for file in\n",
        "                            transcript_files]\n",
        "\n",
        "        audios, transcripts = [], []\n",
        "\n",
        "        for audio_path, transcript_path in zip(audio_paths, transcript_paths):\n",
        "            with open(transcript_path, 'r') as f:\n",
        "                transcript = f.read().strip()\n",
        "                transcripts.append(transcript)\n",
        "\n",
        "        loaded_audios = [torchaudio.load(audio)[0] for audio in audio_paths]\n",
        "        return audio_paths, loaded_audios, transcripts\n",
        "\n",
        "    def get_data(self, split):\n",
        "        if split == 'train':\n",
        "            return self.x_train, self.y_train\n",
        "        elif split == 'val':\n",
        "            return self.x_val, self.y_val\n",
        "        elif split == 'test':\n",
        "            return self.x_test, self.y_test\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid data split '{split}'\")\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n",
        "        waveform = waveform.numpy()\n",
        "\n",
        "        num_channels, num_frames = waveform.shape\n",
        "        time_axis = torch.arange(0, num_frames) / sample_rate\n",
        "\n",
        "        figure, axes = plt.subplots(num_channels, 1)\n",
        "        if num_channels == 1:\n",
        "            axes = [axes]\n",
        "        for c in range(num_channels):\n",
        "            axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
        "            axes[c].grid(True)\n",
        "            if num_channels > 1:\n",
        "                axes[c].set_ylabel(f'Channel {c + 1}')\n",
        "            if xlim:\n",
        "                axes[c].set_xlim(xlim)\n",
        "            if ylim:\n",
        "                axes[c].set_ylim(ylim)\n",
        "        figure.suptitle(title)\n",
        "        plt.show(block=False)\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_mel_spec(waveform):\n",
        "        mel_specgram = torchaudio.transforms.MelSpectrogram(SR, n_mels=N_MELS)(waveform)\n",
        "        return mel_specgram\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_mfcc(mfcc, title=\"MFCC\", xlim=None, ylim=None):\n",
        "        fig, ax = plt.subplots()\n",
        "        im = ax.imshow(mfcc, origin='lower', aspect='auto')\n",
        "        fig.colorbar(im, ax=ax)\n",
        "        ax.set(title=title, xlabel='Time', ylabel='MFCC')\n",
        "        if xlim:\n",
        "            ax.set_xlim(xlim)\n",
        "        if ylim:\n",
        "            ax.set_ylim(ylim)\n",
        "        plt.show(block=False)\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_mel_spec(mel_spec, title=\"Mel Spectrogram\", xlim=None, ylim=None):\n",
        "        mel_spec = mel_spec.squeeze(0).numpy()\n",
        "        fig, ax = plt.subplots()\n",
        "        im = ax.imshow(mel_spec, origin='lower', aspect='auto')\n",
        "        fig.colorbar(im, ax=ax)\n",
        "        ax.set(title=title, xlabel='Time', ylabel='Frequency (Hz)')\n",
        "        if xlim:\n",
        "            ax.set_xlim(xlim)\n",
        "        if ylim:\n",
        "            ax.set_ylim(ylim)\n",
        "        plt.show(block=False)\n",
        "\n",
        "\n",
        "def test_distance_algorithms(data):\n",
        "    \"\"\"\n",
        "    Test the Distances algorithms (DTW & Euclidean) as the most naive\n",
        "    implementations.\n",
        "    \"\"\"\n",
        "    x_train, y_train = data.get_data('train')\n",
        "    x_val, y_val = data.get_data('val')\n",
        "    x_test, y_test = data.get_data('test')\n",
        "\n",
        "    dtw = DTWModel(x_train, y_train)\n",
        "    dtw.add_data(x_val, y_val)\n",
        "    predictions_dtw = dtw.classify_using_DTW_distance(x_test)\n",
        "    print('Predictions:')\n",
        "    print(predictions_dtw[:5])\n",
        "    print('True labels:')\n",
        "    print(y_test[:5])\n",
        "    print('Testing DTW algorithm...')\n",
        "    wer_error = wer(y_test, predictions_dtw)\n",
        "    cer_error = cer(y_test, predictions_dtw)\n",
        "    print(f'DTW Test WER: {wer_error:.4f}')\n",
        "    print(f'DTW Test CER: {cer_error:.4f}')\n",
        "    print('DTW tested successfully')\n"
      ],
      "metadata": {
        "id": "SxmtCmUegorM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoders"
      ],
      "metadata": {
        "id": "3SV88qfigz8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoders\n",
        "def greedy_decoder(output, labels, label_lengths,\n",
        "                   blank_label=28, collapse_repeated=True):\n",
        "    arg_maxes = torch.argmax(output, dim=2)\n",
        "    decodes = []\n",
        "    targets = []\n",
        "    text_transform = TextTransform()\n",
        "    for i, args in enumerate(arg_maxes):\n",
        "        decode = []\n",
        "        target = text_transform.int_to_text(labels[i][:label_lengths[i]].tolist())\n",
        "        targets.append(target)\n",
        "        for j, index in enumerate(args):\n",
        "            if index != blank_label:\n",
        "                if collapse_repeated and j != 0 and index == args[j -1]:\n",
        "                    continue\n",
        "                decode.append(index.item())\n",
        "        pred = text_transform.int_to_text(decode)\n",
        "        decodes.append(pred)\n",
        "        # print(f\"pred:{pred}, real: {target}\")\n",
        "\n",
        "    return decodes, targets\n",
        "\n",
        "\n",
        "def beam_decoder(output, labels, label_lengths, compare=True):\n",
        "    output = output.to('cpu')\n",
        "    beam_decoder = ctc_decoder(lexicon=LEXICON_PATH, tokens=TOKENS_PATH, lm=files.lm, blank_token='|',\n",
        "                               sil_token='SPACE', lm_weight=1, beam_size=100,\n",
        "                               word_score=-1)\n",
        "    beam_search_result = beam_decoder(output.contiguous())\n",
        "    text_transform = TextTransform()\n",
        "    preds, actuals = [], []\n",
        "    for i in range(output.shape[0]):\n",
        "        pred = \" \".join(beam_search_result[i][0].words).strip()\n",
        "        preds.append(pred)\n",
        "\n",
        "        if compare:\n",
        "          actual = text_transform.int_to_text(labels[i][:label_lengths[i]].tolist())\n",
        "          actuals.append(actual)\n",
        "\n",
        "    return preds, actuals\n",
        "\n"
      ],
      "metadata": {
        "id": "UNpfsbTt6Oi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data processing"
      ],
      "metadata": {
        "id": "hRKPCpC8gyJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextTransform:\n",
        "    def __init__(self):\n",
        "        char_map_str = \"\"\"\n",
        "            ' 0\n",
        "            SPACE 1\n",
        "            a 2\n",
        "            b 3\n",
        "            c 4\n",
        "            d 5\n",
        "            e 6\n",
        "            f 7\n",
        "            g 8\n",
        "            h 9\n",
        "            i 10\n",
        "            j 11\n",
        "            k 12\n",
        "            l 13\n",
        "            m 14\n",
        "            n 15\n",
        "            o 16\n",
        "            p 17\n",
        "            q 18\n",
        "            r 19\n",
        "            s 20\n",
        "            t 21\n",
        "            u 22\n",
        "            v 23\n",
        "            w 24\n",
        "            x 25\n",
        "            y 26\n",
        "            z 27\n",
        "            \"\"\"\n",
        "        self.char_map = {}\n",
        "        self.index_map = {}\n",
        "        for line in char_map_str.strip().split('\\n'):\n",
        "            ch, index = line.split()\n",
        "            self.char_map[ch] = int(index)\n",
        "            self.index_map[int(index)] = ch\n",
        "        self.index_map[1] = ' '\n",
        "\n",
        "    def text_to_int(self, text):\n",
        "        int_sequence = []\n",
        "        for c in text:\n",
        "            if c == ' ':\n",
        "                ch = self.char_map['SPACE']\n",
        "            else:\n",
        "                ch = self.char_map[c]\n",
        "            int_sequence.append(ch)\n",
        "        return int_sequence\n",
        "\n",
        "    def int_to_text(self, labels):\n",
        "        string = []\n",
        "        for i in labels:\n",
        "            string.append(self.index_map[i])\n",
        "        return ''.join(string).replace('SPACE', ' ')\n"
      ],
      "metadata": {
        "id": "Lo_obzPXLb8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2ep0TP9KzRH"
      },
      "outputs": [],
      "source": [
        "# data loader\n",
        "class AN4Dataset(Dataset):\n",
        "    def __init__(self, split, transform=None, target_transform=None):\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "        audio_dir = os.path.join(DATA_PATH, split, 'an4', 'wav')\n",
        "        transcript_dir = os.path.join(DATA_PATH, split, 'an4', 'txt')\n",
        "\n",
        "        audio_files = sorted(os.listdir(audio_dir))\n",
        "        transcript_files = sorted(os.listdir(transcript_dir))\n",
        "\n",
        "        audio_paths = [os.path.join(audio_dir, file) for file in audio_files if file.endswith('wav')]\n",
        "        transcript_paths = [os.path.join(transcript_dir, file) for file in\n",
        "                            transcript_files if file.endswith('txt')]\n",
        "\n",
        "        self.audios, self.transcripts = [], []\n",
        "\n",
        "        for audio_path, transcript_path in zip(audio_paths, transcript_paths):\n",
        "            with open(transcript_path, 'r') as f:\n",
        "                transcript = f.read().strip()\n",
        "                self.transcripts.append(transcript)\n",
        "\n",
        "        self.loaded_audios = [torchaudio.load(audio)[0] for audio in audio_paths]\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.loaded_audios)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.loaded_audios[idx], self.transcripts[idx]\n",
        "\n",
        "\n",
        "def data_processing(data, data_type=\"train\"):\n",
        "    inputs, inputs_lengths, labels, labels_length = [], [], [], []\n",
        "\n",
        "    if data_type == \"train\":\n",
        "        transform = nn.Sequential(\n",
        "            torchaudio.transforms.MelSpectrogram(sample_rate=SR, n_mels=N_MELS),\n",
        "            torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
        "            torchaudio.transforms.TimeMasking(time_mask_param=100))\n",
        "    else:\n",
        "        transform = torchaudio.transforms.MelSpectrogram(sample_rate=SR, n_mels=N_MELS)\n",
        "\n",
        "    text_transform = TextTransform()\n",
        "    for (wav, transcript) in data:\n",
        "        spec = transform(wav).squeeze(0).transpose(0, 1)\n",
        "        inputs.append(spec)\n",
        "        inputs_lengths.append(spec.shape[0] // 2)\n",
        "        label = torch.Tensor(text_transform.text_to_int(str(transcript).lower()))\n",
        "        labels.append(label)\n",
        "        labels_length.append(len(label))\n",
        "\n",
        "    max_length = (max(inputs_lengths) * 2) + 1\n",
        "\n",
        "    # Pad tensors and create the big tensor\n",
        "    spectrograms = torch.zeros((len(inputs), max_length, N_MELS))\n",
        "    for i, tensor in enumerate(inputs):\n",
        "        spectrograms[i, :tensor.shape[0], :] = tensor[:, :]\n",
        "\n",
        "    spectrograms = spectrograms.unsqueeze(1).transpose(2, 3)\n",
        "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "    return spectrograms, labels, inputs_lengths, labels_length\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Models"
      ],
      "metadata": {
        "id": "OOci_zcSgvXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicLSTMModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(BasicLSTMModel, self).__init__()\n",
        "        self.rnn = nn.LSTM(input_size=N_MELS, hidden_size=HIDDEN_DIM,\n",
        "                           bidirectional=True, batch_first=True)\n",
        "\n",
        "        self.fc = nn.Linear(HIDDEN_DIM*2, NUM_CLASSES)\n",
        "\n",
        "    def forward(self, x):\n",
        "        sizes = x.size()\n",
        "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
        "        x = x.transpose(1, 2)  # (batch, time, feature)\n",
        "        rnn_output, _ = self.rnn(x)\n",
        "        output = self.fc(rnn_output)\n",
        "\n",
        "        return output\n",
        "\n"
      ],
      "metadata": {
        "id": "mmBxwBRB5ZBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdvanceLSTMModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AdvanceLSTMModel, self).__init__()\n",
        "        # self.vocabulary = vocabulary\n",
        "        # self.lang_model = lang_model\n",
        "\n",
        "        # RNN layers\n",
        "        self.rnn = nn.LSTM(input_size=512, hidden_size=128,\n",
        "                           num_layers=3, batch_first=True, bidirectional=True)\n",
        "\n",
        "        self.layer_norm1 = nn.LayerNorm(512)\n",
        "        self.layer_norm2 = nn.LayerNorm(256)\n",
        "        self.layer_norm3 = nn.LayerNorm(128)\n",
        "        self.layer_norm4 = nn.LayerNorm(5504)\n",
        "        self.dropout = nn.Dropout(0.15)\n",
        "\n",
        "        self.fc1 = nn.Linear(5504, 512)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3)\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 29)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        sizes = x.size()\n",
        "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
        "        x = x.transpose(1, 2)  # (batch, time, feature)\n",
        "        x = self.layer_norm4(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer_norm1(x)\n",
        "        x, _ = self.rnn(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer_norm2(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer_norm3(x)\n",
        "        output = self.fc3(x)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "6Pgyx-hLigqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNLayerNorm(nn.Module):\n",
        "    def __init__(self, n_feats):\n",
        "        super(CNNLayerNorm, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(2, 3).contiguous()\n",
        "        x = self.layer_norm(x)\n",
        "        return x.transpose(2, 3).contiguous()\n",
        "\n",
        "\n",
        "class ResidualCNN(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel, stride, dropout,\n",
        "                 n_feats):\n",
        "        super(ResidualCNN, self).__init__()\n",
        "\n",
        "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride,\n",
        "                              padding=kernel // 2)\n",
        "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride,\n",
        "                              padding=kernel // 2)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
        "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = self.layer_norm1(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.cnn1(x)\n",
        "        x = self.layer_norm2(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.cnn2(x)\n",
        "        x += residual\n",
        "        return x\n",
        "\n",
        "\n",
        "class BidirectionalGRU(nn.Module):\n",
        "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
        "        super(BidirectionalGRU, self).__init__()\n",
        "\n",
        "        self.BiGRU = nn.GRU(\n",
        "            input_size=rnn_dim, hidden_size=hidden_size,\n",
        "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
        "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        x = F.gelu(x)\n",
        "        x, _ = self.BiGRU(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FinalASRModel(nn.Module):\n",
        "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class,\n",
        "                 n_feats, stride, dropout):\n",
        "        super(FinalASRModel, self).__init__()\n",
        "        n_feats = n_feats // 2\n",
        "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3 // 2)\n",
        "        self.rescnn_layers = nn.Sequential(*[\n",
        "            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout,\n",
        "                        n_feats=n_feats)\n",
        "            for _ in range(n_cnn_layers)\n",
        "        ])\n",
        "        self.fully_connected = nn.Linear(n_feats * 32, rnn_dim)\n",
        "        self.birnn_layers = nn.Sequential(*[\n",
        "            BidirectionalGRU(rnn_dim=rnn_dim if i == 0 else rnn_dim * 2,\n",
        "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i == 0)\n",
        "            for i in range(n_rnn_layers)\n",
        "        ])\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(rnn_dim * 2, rnn_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(rnn_dim, n_class)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x)\n",
        "        x = self.rescnn_layers(x)\n",
        "        sizes = x.size()\n",
        "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])\n",
        "         # (batch, feature, time)\n",
        "        x = x.transpose(1, 2)  # (batch, time, feature)\n",
        "        x = self.fully_connected(x)\n",
        "        x = self.birnn_layers(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "ceCaO2YleiRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and test loops"
      ],
      "metadata": {
        "id": "PiTrhIVWyLYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, path):\n",
        "    \"\"\"\n",
        "    Saves a pytorch models to the given path.\n",
        "    \"\"\"\n",
        "    torch.save(model.state_dict(), '{}'.format(path))\n",
        "\n",
        "\n",
        "def load_model(model, path):\n",
        "    \"\"\"\n",
        "    Loads a pytorch models from the given path. The models should already by\n",
        "    created (e.g. by calling the constructor) and should be passed as an argument.\n",
        "    \"\"\"\n",
        "    model.load_state_dict(torch.load('{}'.format(path)))\n",
        "\n",
        "\n",
        "\n",
        "def test(model, test_loader, criterion, experiment, counter=0, isEval=False,\n",
        "         decoder=greedy_decoder):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    t_loss, test_cer, test_wer = [], [], []\n",
        "    with experiment.test():\n",
        "        with torch.no_grad():\n",
        "            for i, _data in enumerate(test_loader):\n",
        "                spectrograms, labels, input_lengths, label_lengths = _data\n",
        "                spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "                output = model(spectrograms)  # (batch, time, n_class)\n",
        "                output = F.log_softmax(output, dim=2)\n",
        "                output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "                loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "                t_loss.append(loss.item())\n",
        "\n",
        "                decoded_preds, decoded_targets = decoder(output.transpose(0, 1), labels, label_lengths)\n",
        "                for j in range(len(decoded_preds)):\n",
        "                    test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
        "                    test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
        "        avg_cer = sum(test_cer)/len(test_cer)\n",
        "        avg_wer = sum(test_wer)/len(test_wer)\n",
        "        avg_loss = sum(t_loss)/len(t_loss)\n",
        "        if isEval:\n",
        "          experiment.log_metric('test_loss', avg_loss, step=counter)\n",
        "          experiment.log_metric('cer', avg_cer, step=counter)\n",
        "          experiment.log_metric('wer', avg_wer, step=counter)\n",
        "        else:\n",
        "          print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(avg_loss, avg_cer, avg_wer))\n",
        "\n",
        "\n",
        "def train(model, train_loader, val_loader, criterion, experiment):\n",
        "    data_len = len(train_loader.dataset)\n",
        "    model.train()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), LEARNING_RATE)\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=LEARNING_RATE,\n",
        "                                            steps_per_epoch=int(len(train_loader)),\n",
        "                                            epochs=N_EPOCHS,\n",
        "                                            anneal_strategy='linear')\n",
        "    with experiment.train():\n",
        "      counter = 0\n",
        "      for epoch in range(N_EPOCHS):\n",
        "          model.train()\n",
        "          if (epoch + 1) % 10 == 0:\n",
        "              save_model(model, CTC_MODEL_PATH)\n",
        "\n",
        "          e_loss = 0\n",
        "          losses = []\n",
        "          for batch_idx, _data in enumerate(train_loader):\n",
        "              spectrograms, labels, input_lengths, label_lengths = _data\n",
        "              spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "              optimizer.zero_grad()\n",
        "              output = model(spectrograms)  # (batch, time, n_class)\n",
        "              output = F.log_softmax(output, dim=2)\n",
        "              output = output.transpose(0, 1)  # (time, batch, n_class)\n",
        "\n",
        "              loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "              loss.backward()\n",
        "\n",
        "              # log it\n",
        "              optimizer.step()\n",
        "              scheduler.step()\n",
        "              e_loss += loss.item()\n",
        "              losses.append(loss.item())\n",
        "\n",
        "          print(f\"Train Epoch: {epoch}, loss = {e_loss}\")\n",
        "          experiment.log_metric('loss', sum(losses)/len(losses), step=epoch+1)\n",
        "          experiment.log_metric('learning_rate', scheduler.get_last_lr(),\n",
        "                                step=epoch+1)\n",
        "\n",
        "          test(model, val_loader, criterion, experiment, epoch+1, True)\n",
        "\n",
        "    save_model(model, CTC_MODEL_PATH)\n"
      ],
      "metadata": {
        "id": "8olxeA0FK4SA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Telemtry init"
      ],
      "metadata": {
        "id": "qd3L6fpvyPSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setp up logging platform\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "comet_api_key = \"\" # todo - add\n",
        "project_name = \"67455 Introduction to Speech Processing - Final project\"\n",
        "experiment_name = \"\" # todo - add\n",
        "\n",
        "if comet_api_key:\n",
        "  experiment = Experiment(api_key=comet_api_key, project_name=project_name, parse_args=False)\n",
        "  experiment.set_name(experiment_name)\n",
        "  experiment.display()\n",
        "else:\n",
        "  experiment = Experiment(api_key='dummy_key', disabled=True)"
      ],
      "metadata": {
        "id": "8iptwPXo2QAg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c54f8b2-d293-4ec9-be8d-ca2dbbe48f36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "plotting"
      ],
      "metadata": {
        "id": "hmN-XsehyT-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_for_plot(dataset, model, num):\n",
        "    transform = torchaudio.transforms.MelSpectrogram(sample_rate=SR, n_mels=N_MELS)\n",
        "    for i in range(num):\n",
        "        sample, target = dataset.__getitem__(i)\n",
        "        spec = transform(sample).squeeze(0).transpose(0, 1)\n",
        "        spectrograms = torch.stack([spec], dim=0)\n",
        "        spectrograms = spectrograms.unsqueeze(1).transpose(2, 3)\n",
        "\n",
        "        model.eval()\n",
        "        spectrograms = spectrograms.to(device)\n",
        "        output = model(spectrograms)\n",
        "        output = output.to('cpu')\n",
        "        beam_decoder = ctc_decoder(lexicon=LEXICON_PATH, tokens=TOKENS_PATH,\n",
        "                                   lm=files.lm, blank_token='|',\n",
        "                                  sil_token='SPACE')\n",
        "        beam_search_result = beam_decoder(output.contiguous())\n",
        "        pred = \" \".join(beam_search_result[0][0].words).strip()\n",
        "        tokens = [c for c in pred]\n",
        "        tokens.insert(0, ' ')\n",
        "        tokens.append(' ')\n",
        "\n",
        "        plot_alignments(sample[0], output, tokens,\n",
        "                        beam_search_result[0][0].timesteps, SR)\n",
        "\n",
        "def plot_alignments(waveform, emission, tokens, timesteps, sample_rate):\n",
        "\n",
        "    t = torch.arange(waveform.size(0)) / sample_rate\n",
        "    ratio = waveform.size(0) / emission.size(1) / sample_rate\n",
        "\n",
        "    chars = []\n",
        "    words = []\n",
        "    word_start = None\n",
        "    for token, timestep in zip(tokens, timesteps * ratio):\n",
        "        if token == \" \":\n",
        "            if word_start is not None:\n",
        "                words.append((word_start, timestep))\n",
        "            word_start = None\n",
        "        else:\n",
        "            chars.append((token, timestep))\n",
        "            if word_start is None:\n",
        "                word_start = timestep\n",
        "\n",
        "    fig, axes = plt.subplots(1, 1)\n",
        "\n",
        "    def _plot(ax, xlim):\n",
        "        ax.plot(t, waveform)\n",
        "        for token, timestep in chars:\n",
        "            ax.annotate(token.upper(), (timestep, 0.4))\n",
        "        for word_start, word_end in words:\n",
        "            ax.axvspan(word_start, word_end, alpha=0.1, color=\"red\")\n",
        "        ax.set_ylim(-0.6, 0.7)\n",
        "        ax.set_yticks([0])\n",
        "        ax.grid(True, axis=\"y\")\n",
        "\n",
        "    _plot(axes, (0.0, (timesteps * ratio)[-1]+ 3/2))\n",
        "    axes.set_xlabel(\"Time (Sec)\")\n",
        "    fig.show()\n"
      ],
      "metadata": {
        "id": "LXrcbb72yVB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "real time testing"
      ],
      "metadata": {
        "id": "bPLMrHz28Xpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub\n",
        "!apt-get install -y ffmpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Cot-6ZD7lvY",
        "outputId": "bede1a10-83ea-4d9a-9c16-e505612b5927"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment\n",
        "import numpy as np\n",
        "\n",
        "def record_audio(duration):\n",
        "    print(f\"Recording audio for {duration} seconds...\")\n",
        "    audio = AudioSegment.silent(duration * 1000)\n",
        "    audio = audio.set_channels(2)\n",
        "    audio.export(\"temp_audio.wav\", format=\"wav\")\n",
        "    print(audio)\n",
        "record_audio(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwnpNWWi7tOL",
        "outputId": "83d34fde-1821-444e-f5e7-1ed4af2880fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recording audio for 5 seconds...\n",
            "<pydub.audio_segment.AudioSegment object at 0x7adc70e6a560>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "driver"
      ],
      "metadata": {
        "id": "f2Vv25S7yWe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print('--- Start running ---')\n",
        "    train_data_set = AN4Dataset('train')\n",
        "    val_data_set = AN4Dataset('val')\n",
        "    test_data_set = AN4Dataset('test')\n",
        "    train_loader = DataLoader(dataset=train_data_set,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              shuffle=True,\n",
        "                              collate_fn=lambda x: data_processing(x, 'train'))\n",
        "    val_loader = DataLoader(dataset=val_data_set,\n",
        "                             batch_size=BATCH_SIZE,\n",
        "                             shuffle=False,\n",
        "                             collate_fn=lambda x: data_processing(x, 'val'))\n",
        "    test_loader = DataLoader(dataset=test_data_set,\n",
        "                             batch_size=BATCH_SIZE,\n",
        "                             shuffle=False,\n",
        "                             collate_fn=lambda x: data_processing(x, 'val'))\n",
        "\n",
        "    ctc_lstm = FinalASRModel(N_CNN_LAYERS, N_RNN_LAYERS, RNN_DIM, N_CLASSES,\n",
        "                             N_MELS, STRIDE,DROPOUT).to(device)\n",
        "\n",
        "    lossFunc = nn.CTCLoss(blank=28, zero_infinity=True).to(device)\n",
        "    load_model(ctc_lstm, CTC_MODEL_PATH)\n",
        "\n",
        "    # train(ctc_lstm, train_loader, val_loader, lossFunc, experiment)\n",
        "    # test(ctc_lstm, test_loader, lossFunc, experiment, decoder=beam_decoder)\n",
        "\n",
        "    # plot aligments from test\n",
        "    # parse_for_plot(test_data_set, ctc_lstm, 10)\n",
        "    print('-- Finished running ---')\n",
        "    record_audio(2)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "-vj7zzNxK-h-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "44cb3924-2a95-4e67-d655-02e9f8c69088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Start running ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-936e014e2d2f>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-936e014e2d2f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--- Start running ---'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_data_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAN4Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mval_data_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAN4Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtest_data_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAN4Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-bebac24b618c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, split, transform, target_transform)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maudio_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranscript_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranscript_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m                 \u001b[0mtranscript\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranscripts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/codecs.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, errors)\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0mbyte\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \"\"\"\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'strict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0mIncrementalDecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0;31m# undecoded input that is kept between calls to decode()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test it yourself !"
      ],
      "metadata": {
        "id": "fPTLQDoj7oAx"
      }
    }
  ]
}