{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1LMUXPYbAzHRGSKhFibuWh3B52tS3F4aG",
      "authorship_tag": "ABX9TyOSZ9SUhs3yLnaKV9jMXCqU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dor890/Speech-Processing/blob/master/FinProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Numerical constants\n",
        "SR = 16000\n",
        "FILE_2CHECK = 13\n",
        "HOP_LEN = 160\n",
        "N_FFT = 400\n",
        "N_MELS = 128  # 128 for Mel_Spec, 23 for MFCC\n",
        "N_MFCC = 13\n",
        "N_EPOCHS = 200\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.0004\n",
        "WEIGHT_DECAY = 0.001\n",
        "NUM_LAYERS = 12\n",
        "HIDDEN_DIM = 64\n",
        "EMBED_DIM = 300\n",
        "NUM_CLASSES = 29\n",
        "TIME = 513  # 513 for Mel_Spec, 641 for MFCC\n",
        "PAD_TOKEN = 0\n",
        "SEQ_LEN = 3\n",
        "DROPOUT = 0.15\n",
        "\n",
        "# Strings constants\n",
        "CTC_MODEL_PATH = 'models/ctc_model.pth'\n",
        "LANG_MODEL_PATH = 'models/lang_model.pth'\n",
        "DATA_PATH = '/content/drive/MyDrive/an4'\n",
        "\n",
        "hparams = {\n",
        "    \"n_cnn_layers\": 2,\n",
        "    \"n_rnn_layers\": 2,\n",
        "    \"rnn_dim\": 512,\n",
        "    \"n_class\": 29,\n",
        "    \"n_feats\": 128,\n",
        "    \"stride\": 2,\n",
        "    \"dropout\": DROPOUT,\n",
        "    \"learning_rate\": LEARNING_RATE,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"epochs\": N_EPOCHS\n",
        "}"
      ],
      "metadata": {
        "id": "r_nRg0LDLT0O"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "class TextTransform:\n",
        "    \"\"\"Maps characters to integers and vice versa\"\"\"\n",
        "    \"\"\"Maps characters to integers and vice versa\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        char_map_str = \"\"\"\n",
        "            ' 0\n",
        "            <SPACE> 1\n",
        "            a 2\n",
        "            b 3\n",
        "            c 4\n",
        "            d 5\n",
        "            e 6\n",
        "            f 7\n",
        "            g 8\n",
        "            h 9\n",
        "            i 10\n",
        "            j 11\n",
        "            k 12\n",
        "            l 13\n",
        "            m 14\n",
        "            n 15\n",
        "            o 16\n",
        "            p 17\n",
        "            q 18\n",
        "            r 19\n",
        "            s 20\n",
        "            t 21\n",
        "            u 22\n",
        "            v 23\n",
        "            w 24\n",
        "            x 25\n",
        "            y 26\n",
        "            z 27\n",
        "            \"\"\"\n",
        "        self.char_map = {}\n",
        "        self.index_map = {}\n",
        "        for line in char_map_str.strip().split('\\n'):\n",
        "            ch, index = line.split()\n",
        "            self.char_map[ch] = int(index)\n",
        "            self.index_map[int(index)] = ch\n",
        "        self.index_map[1] = ' '\n",
        "\n",
        "    def text_to_int(self, text):\n",
        "        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
        "        int_sequence = []\n",
        "        for c in text:\n",
        "            if c == ' ':\n",
        "                ch = self.char_map['<SPACE>']\n",
        "            else:\n",
        "                ch = self.char_map[c]\n",
        "            int_sequence.append(ch)\n",
        "        return int_sequence\n",
        "\n",
        "    def int_to_text(self, labels):\n",
        "        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n",
        "        string = []\n",
        "        for i in labels:\n",
        "            string.append(self.index_map[i])\n",
        "        return ''.join(string).replace('<SPACE>', ' ')\n",
        "\n",
        "\n",
        "def gd(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n",
        "    arg_maxes = torch.argmax(output, dim=2)\n",
        "    decodes = []\n",
        "    targets = []\n",
        "    text_transform = TextTransform()\n",
        "    for i, args in enumerate(arg_maxes):\n",
        "        decode = []\n",
        "        targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
        "        for j, index in enumerate(args):\n",
        "            if index != blank_label:\n",
        "                if collapse_repeated and j != 0 and index == args[j -1]:\n",
        "                    continue\n",
        "                decode.append(index.item())\n",
        "        decodes.append(text_transform.int_to_text(decode))\n",
        "    return decodes, targets\n"
      ],
      "metadata": {
        "id": "Lo_obzPXLb8a"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "n2ep0TP9KzRH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Data:\n",
        "    def __init__(self):\n",
        "        print('Loading data...')\n",
        "        self.data_dir = 'an4'\n",
        "        self.x_train_paths, self.x_train, self.y_train = self.load_data('train')\n",
        "        self.x_val_paths, self.x_val, self.y_val = self.load_data('val')\n",
        "        self.x_test_paths, self.x_test, self.y_test = self.load_data('test')\n",
        "\n",
        "        # Print the first example in the training set\n",
        "        # print(f\"First train file path: {self.x_train_paths[FILE_2CHECK]}\")\n",
        "        # print(f\"Transcription: {self.y_train[FILE_2CHECK]}\")\n",
        "        # print(f\"Preview first train file: {self.x_train[FILE_2CHECK]}\")\n",
        "        # self.plot_waveform(self.x_train[0], sample_rate=SR)\n",
        "        # self.plot_mfcc(extract_features(self.x_train[FILE_2CHECK]))\n",
        "        # self.plot_mel_spec(self.extract_mel_spec(self.x_train[FILE_2CHECK]))\n",
        "        print('Data loaded successfully')\n",
        "\n",
        "    def load_data(self, split):\n",
        "        \"\"\"\n",
        "        Load the data from the provided 'an4' folder, and split it into train, dev, and test sets.\n",
        "        \"\"\"\n",
        "        audio_dir = os.path.join(self.data_dir, split, 'an4', 'wav')\n",
        "        transcript_dir = os.path.join(self.data_dir, split, 'an4', 'txt')\n",
        "\n",
        "        audio_files = sorted(os.listdir(audio_dir))\n",
        "        transcript_files = sorted(os.listdir(transcript_dir))\n",
        "\n",
        "        audio_paths = [os.path.join(audio_dir, file) for file in audio_files]\n",
        "        transcript_paths = [os.path.join(transcript_dir, file) for file in\n",
        "                            transcript_files]\n",
        "\n",
        "        audios, transcripts = [], []\n",
        "\n",
        "        for audio_path, transcript_path in zip(audio_paths, transcript_paths):\n",
        "            with open(transcript_path, 'r') as f:\n",
        "                transcript = f.read().strip()\n",
        "                transcripts.append(transcript)\n",
        "\n",
        "        loaded_audios = [torchaudio.load(audio)[0] for audio in audio_paths]\n",
        "        return audio_paths, loaded_audios, transcripts\n",
        "\n",
        "    def get_data(self, split):\n",
        "        if split == 'train':\n",
        "            return self.x_train, self.y_train\n",
        "        elif split == 'val':\n",
        "            return self.x_val, self.y_val\n",
        "        elif split == 'test':\n",
        "            return self.x_test, self.y_test\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid data split '{split}'\")\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n",
        "        waveform = waveform.numpy()\n",
        "\n",
        "        num_channels, num_frames = waveform.shape\n",
        "        time_axis = torch.arange(0, num_frames) / sample_rate\n",
        "\n",
        "        figure, axes = plt.subplots(num_channels, 1)\n",
        "        if num_channels == 1:\n",
        "            axes = [axes]\n",
        "        for c in range(num_channels):\n",
        "            axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
        "            axes[c].grid(True)\n",
        "            if num_channels > 1:\n",
        "                axes[c].set_ylabel(f'Channel {c + 1}')\n",
        "            if xlim:\n",
        "                axes[c].set_xlim(xlim)\n",
        "            if ylim:\n",
        "                axes[c].set_ylim(ylim)\n",
        "        figure.suptitle(title)\n",
        "        plt.show(block=False)\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_mel_spec(waveform):\n",
        "        mel_specgram = torchaudio.transforms.MelSpectrogram(SR, n_mels=N_MELS)(waveform)\n",
        "        return mel_specgram\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_mfcc(mfcc, title=\"MFCC\", xlim=None, ylim=None):\n",
        "        fig, ax = plt.subplots()\n",
        "        im = ax.imshow(mfcc, origin='lower', aspect='auto')\n",
        "        fig.colorbar(im, ax=ax)\n",
        "        ax.set(title=title, xlabel='Time', ylabel='MFCC')\n",
        "        if xlim:\n",
        "            ax.set_xlim(xlim)\n",
        "        if ylim:\n",
        "            ax.set_ylim(ylim)\n",
        "        plt.show(block=False)\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_mel_spec(mel_spec, title=\"Mel Spectrogram\", xlim=None, ylim=None):\n",
        "        mel_spec = mel_spec.squeeze(0).numpy()\n",
        "        fig, ax = plt.subplots()\n",
        "        im = ax.imshow(mel_spec, origin='lower', aspect='auto')\n",
        "        fig.colorbar(im, ax=ax)\n",
        "        ax.set(title=title, xlabel='Time', ylabel='Frequency (Hz)')\n",
        "        if xlim:\n",
        "            ax.set_xlim(xlim)\n",
        "        if ylim:\n",
        "            ax.set_ylim(ylim)\n",
        "        plt.show(block=False)\n",
        "\n",
        "\n",
        "class AN4Dataset(Dataset):\n",
        "    def __init__(self, split, transform=None, target_transform=None):\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "        audio_dir = os.path.join(DATA_PATH, split, 'an4', 'wav')\n",
        "        transcript_dir = os.path.join(DATA_PATH, split, 'an4', 'txt')\n",
        "\n",
        "        audio_files = sorted(os.listdir(audio_dir))\n",
        "        transcript_files = sorted(os.listdir(transcript_dir))\n",
        "\n",
        "        assert len(audio_dir) == len(transcript_dir)\n",
        "        # for i in range(len(audio_files)):\n",
        "        #     a_name = audio_files[i].split(\".\")[0]\n",
        "        #     t_name = transcript_files[i].split(\".\")[0]\n",
        "\n",
        "        #     # assert a_name == t_name\n",
        "\n",
        "        audio_paths = [os.path.join(audio_dir, file) for file in audio_files if file.endswith('wav')]\n",
        "        transcript_paths = [os.path.join(transcript_dir, file) for file in\n",
        "                            transcript_files if file.endswith('txt')]\n",
        "\n",
        "        self.audios, self.transcripts = [], []\n",
        "\n",
        "        for audio_path, transcript_path in zip(audio_paths, transcript_paths):\n",
        "            with open(transcript_path, 'r') as f:\n",
        "                transcript = f.read().strip()\n",
        "                self.transcripts.append(transcript)\n",
        "\n",
        "        self.loaded_audios = [torchaudio.load(audio)[0] for audio in audio_paths]\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.loaded_audios)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.loaded_audios[idx], self.transcripts[idx]\n",
        "\n",
        "\n",
        "def data_processing(data, data_type=\"train\"):\n",
        "    inputs, inputs_lengths, labels, labels_length = [], [], [], []\n",
        "\n",
        "    if data_type == \"train\":\n",
        "        transform = nn.Sequential(\n",
        "            torchaudio.transforms.MelSpectrogram(sample_rate=SR, n_mels=N_MELS),\n",
        "            torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
        "            torchaudio.transforms.TimeMasking(time_mask_param=100))\n",
        "    else:\n",
        "        transform = torchaudio.transforms.MelSpectrogram()\n",
        "\n",
        "    text_transform = TextTransform()\n",
        "    for (wav, transcript) in data:\n",
        "        spec = transform(wav).squeeze(0).transpose(0, 1)\n",
        "        inputs.append(spec)\n",
        "        inputs_lengths.append(spec.shape[0] // 2)\n",
        "        label = torch.Tensor(text_transform.text_to_int(str(transcript).lower()))\n",
        "        labels.append(label)\n",
        "        labels_length.append(len(label))\n",
        "\n",
        "\n",
        "    max_length = (max(inputs_lengths) * 2) + 1\n",
        "    # Pad tensors and create the big tensor\n",
        "    spectrograms = torch.zeros((len(inputs), max_length, N_MELS))\n",
        "    for i, tensor in enumerate(inputs):\n",
        "        spectrograms[i, :tensor.shape[0], :] = tensor[:, :]\n",
        "\n",
        "    spectrograms = spectrograms.unsqueeze(1).transpose(2, 3)\n",
        "    # spectrograms = nn.utils.rnn.pad_sequence(inputs, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
        "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "    return spectrograms, labels, inputs_lengths, labels_length\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ceCaO2YleiRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchaudio.models.decoder import ctc_decoder\n",
        "from torchaudio.models.decoder._ctc_decoder import download_pretrained_files\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "files = download_pretrained_files(\"librispeech-4-gram\")\n",
        "\n",
        "\n",
        "class CNNLayerNorm(nn.Module):\n",
        "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
        "\n",
        "    def __init__(self, n_feats):\n",
        "        super(CNNLayerNorm, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x (batch, channel, feature, time)\n",
        "        x = x.transpose(2, 3).contiguous()  # (batch, channel, time, feature)\n",
        "        x = self.layer_norm(x)\n",
        "        return x.transpose(2, 3).contiguous()  # (batch, channel, feature, time)\n",
        "\n",
        "\n",
        "class ResidualCNN(nn.Module):\n",
        "    \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n",
        "        except with layer norm instead of batch norm\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel, stride, dropout,\n",
        "                 n_feats):\n",
        "        super(ResidualCNN, self).__init__()\n",
        "\n",
        "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride,\n",
        "                              padding=kernel // 2)\n",
        "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride,\n",
        "                              padding=kernel // 2)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
        "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x  # (batch, channel, feature, time)\n",
        "        x = self.layer_norm1(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.cnn1(x)\n",
        "        x = self.layer_norm2(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.cnn2(x)\n",
        "        x += residual\n",
        "        return x  # (batch, channel, feature, time)\n",
        "\n",
        "\n",
        "class BidirectionalGRU(nn.Module):\n",
        "\n",
        "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
        "        super(BidirectionalGRU, self).__init__()\n",
        "\n",
        "        self.BiGRU = nn.GRU(\n",
        "            input_size=rnn_dim, hidden_size=hidden_size,\n",
        "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
        "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        x = F.gelu(x)\n",
        "        x, _ = self.BiGRU(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SpeechRecognitionModel(nn.Module):\n",
        "    \"\"\"Speech Recognition Model Inspired by DeepSpeech 2\"\"\"\n",
        "\n",
        "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim,\n",
        "                 n_class, n_feats,\n",
        "                 stride=2, dropout=0.1):\n",
        "        super(SpeechRecognitionModel, self).__init__()\n",
        "        # self.vocabulary = vocabulary\n",
        "        n_feats = n_feats // 2\n",
        "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride,\n",
        "                             padding=3 // 2)  # cnn for extracting heirachal features\n",
        "\n",
        "        # n residual cnn layers with filter size of 32\n",
        "        self.rescnn_layers = nn.Sequential(*[\n",
        "            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout,\n",
        "                        n_feats=n_feats)\n",
        "            for _ in range(n_cnn_layers)\n",
        "        ])\n",
        "        self.fully_connected = nn.Linear(n_feats * 32, rnn_dim)\n",
        "        self.birnn_layers = nn.Sequential(*[\n",
        "            BidirectionalGRU(rnn_dim=rnn_dim if i == 0 else rnn_dim * 2,\n",
        "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i == 0)\n",
        "            for i in range(n_rnn_layers)\n",
        "        ])\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(rnn_dim * 2, rnn_dim),  # birnn returns rnn_dim*2\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(rnn_dim, n_class)\n",
        "        )\n",
        "\n",
        "        # Decoders\n",
        "        # self.greedy_decoder = GreedyDecoder(vocabulary.translator.values())\n",
        "        # self.beam_decoder = ctc_decoder(lexicon='lexicon.txt',\n",
        "        #                                 tokens='tokens.txt', lm=None)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x)\n",
        "        x = self.rescnn_layers(x)\n",
        "        sizes = x.size()\n",
        "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
        "        x = x.transpose(1, 2)  # (batch, time, feature)\n",
        "        x = self.fully_connected(x)\n",
        "        x = self.birnn_layers(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def save_model(model, path):\n",
        "    \"\"\"\n",
        "    Saves a pytorch models to the given path.\n",
        "    \"\"\"\n",
        "    torch.save(model.state_dict(), '{}'.format(path))\n",
        "\n",
        "\n",
        "def load_model(model, path):\n",
        "    \"\"\"\n",
        "    Loads a pytorch models from the given path. The models should already by\n",
        "    created (e.g. by calling the constructor) and should be passed as an argument.\n",
        "    \"\"\"\n",
        "    model.load_state_dict(torch.load('{}'.format(path)))\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "def extract_features(wavs, is_train=False):\n",
        "    \"\"\"\n",
        "    Extract MFCC features from the given audios batch.\n",
        "    More ideas: try Time Domain / STFT / Mel Spectrogram\n",
        "    \"\"\"\n",
        "    spectrograms, input_lengths = [], []\n",
        "\n",
        "    # MFCC Transform\n",
        "    # transform = torchaudio.transforms.MFCC(\n",
        "    #     sample_rate=SR, n_mfcc=N_MFCC)\n",
        "    # mfcc_batch = mfcc_transform(wavs).squeeze()\n",
        "    # mfcc_batch = mfcc_batch.permute(0, 2, 1)\n",
        "    # return mfcc_batch\n",
        "\n",
        "    # transform = torchaudio.transforms.MelSpectrogram(SR)\n",
        "    # mel_batch = transform(wavs).squeeze()\n",
        "    # mel_batch = mel_batch.permute(0, 2, 1)\n",
        "    # return mel_batch\n",
        "    if is_train:\n",
        "        transform = nn.Sequential(\n",
        "            torchaudio.transforms.MelSpectrogram(sample_rate=SR, n_mels=N_MELS),\n",
        "            torchaudio.transforms.FrequencyMasking(freq_mask_param=15),\n",
        "            torchaudio.transforms.TimeMasking(time_mask_param=35))\n",
        "    else:\n",
        "        transform = torchaudio.transforms.MelSpectrogram()\n",
        "\n",
        "    for wav in wavs:\n",
        "        spec = transform(wav).squeeze(0).transpose(0, 1)\n",
        "        spectrograms.append(spec)\n",
        "        input_lengths.append(spec.shape[0] // 2)\n",
        "\n",
        "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
        "\n",
        "    # mel_batch = mel_batch.permute(0, 2, 1)  # (batch, mel, timeFrame)\n",
        "    return spectrograms, torch.Tensor(input_lengths).long()\n",
        "\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    \"\"\"\n",
        "    A basic LSTM models for speech recognition.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocabulary, lang_model=None):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.vocabulary = vocabulary\n",
        "        self.lang_model = lang_model\n",
        "\n",
        "        # RNN layers\n",
        "        self.rnn = nn.LSTM(input_size=4032, hidden_size=HIDDEN_DIM,\n",
        "                           num_layers=NUM_LAYERS, batch_first=True)\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(HIDDEN_DIM, NUM_CLASSES)\n",
        "\n",
        "        # Decoders\n",
        "        # self.greedy_decoder = GreedyDecoder(vocabulary.translator.values())\n",
        "        # self.beam_decoder = ctc_decoder(lexicon='lexicon.txt',\n",
        "        #                                 tokens='tokens.txt', lm=files.lm)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        sizes = x.size()\n",
        "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
        "        x = x.transpose(1, 2)  # (batch, time, feature)\n",
        "        rnn_output, _ = self.rnn(x)\n",
        "        output = self.fc(rnn_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "def predict(model, feats):\n",
        "    \"\"\"\n",
        "    Predicts a batch of waveforms using the given models.\n",
        "    \"\"\"\n",
        "    emission = model(feats)\n",
        "    greedy_result = model.greedy_decoder(emission)\n",
        "    # beam_search_result = model.beam_decoder(emission)\n",
        "    return greedy_result\n",
        "\n",
        "\n",
        "def test(model, test_loader, criterion):\n",
        "    print('\\nevaluating...')\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, _data in enumerate(test_loader):\n",
        "            spectrograms, labels, input_lengths, label_lengths = _data\n",
        "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "            output = model(spectrograms)  # (batch, time, n_class)\n",
        "            output = F.log_softmax(output, dim=2).to(device)\n",
        "            output = output.transpose(0, 1)  # (time, batch, n_class)\n",
        "\n",
        "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "            test_loss += loss.item() / len(test_loader)\n",
        "            # arg_maxes = torch.argmax(output.transpose(0, 1), dim=2)\n",
        "            decoded_preds, decoded_targets = gd(output.transpose(0, 1), labels, label_lengths)\n",
        "            print(decoded_preds, decoded_targets)\n",
        "\n",
        "\n",
        "def train_all_data(model, train_loader, criterion):\n",
        "    data_len = len(train_loader.dataset)\n",
        "    optimizer = torch.optim.RMSprop(model.parameters(), LEARNING_RATE)\n",
        "    # scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=LEARNING_RATE,\n",
        "    #                                           steps_per_epoch=data_len,\n",
        "    #                                           epochs=N_EPOCHS,\n",
        "    #                                           anneal_strategy='linear')\n",
        "    model.train()\n",
        "    model = model.to(device)\n",
        "    for epoch in range(N_EPOCHS):\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            save_model(model, CTC_MODEL_PATH)\n",
        "        e_loss = 0\n",
        "        for batch_idx, _data in enumerate(train_loader):\n",
        "            spectrograms, labels, input_lengths, label_lengths = _data\n",
        "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(spectrograms)  # (batch, time, n_class)\n",
        "            output = F.log_softmax(output, dim=2)\n",
        "            output = output.transpose(0, 1)  # (time, batch, n_class)\n",
        "\n",
        "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # scheduler.step()\n",
        "            e_loss += loss.item()\n",
        "        print(f\"Train Epoch: {epoch}, loss = {e_loss}\")\n",
        "\n",
        "    save_model(model, CTC_MODEL_PATH)\n"
      ],
      "metadata": {
        "id": "8olxeA0FK4SA"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "# from jiwer import wer, cer\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def evaluate(model, x_test, y_test):\n",
        "    \"\"\"\n",
        "    Evaluate the models over the test set.\n",
        "    \"\"\"\n",
        "    predictions, targets = [], []\n",
        "\n",
        "    for i, batch_start in tqdm(enumerate(range(0, len(x_test), BATCH_SIZE))):\n",
        "        batch = x_test[batch_start:batch_start + BATCH_SIZE]\n",
        "        # feats = torch.zeros((len(batch), 1, MAX_LEN))\n",
        "        # for i, tensor in enumerate(batch):\n",
        "        #     padded_tensor = torch.cat(\n",
        "        #         [tensor, torch.zeros((1, MAX_LEN-tensor.size(1)))], dim=1)\n",
        "        #     big_tensor[i] = padded_tensor\n",
        "        feats, _ = ctc_model.extract_features(batch)\n",
        "        batch_preds = ctc_model.predict(model, feats)\n",
        "        for j in range(len(batch_preds)):\n",
        "            pred_tokens = model.beam_decoder.idxs_to_tokens(batch_preds[j][0].tokens)\n",
        "            if j % 50 == 0:\n",
        "                print(f'True transcription: {y_test[batch_start + j]}')\n",
        "                print(f'Predicted transcription: {pred_tokens}')\n",
        "            predictions.append(pred_tokens)\n",
        "            targets.append(y_test[batch_start + j])\n",
        "            # plot_alignments(batch[j],\n",
        "            #                 models(models.extract_features(batch[j])),\n",
        "            #                 pred_tokens, batch_preds[j].timesteps)\n",
        "\n",
        "    wer_error = wer(targets, predictions)\n",
        "    cer_error = cer(targets, predictions)\n",
        "    return wer_error, cer_error\n",
        "\n",
        "\n",
        "def plot_alignments(waveform, emission, tokens, timesteps):\n",
        "    \"\"\"\n",
        "    Plots the alignment between the waveform and the predicted transcription.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(32, 10))\n",
        "    ax.plot(waveform)\n",
        "\n",
        "    ratio = waveform.shape[0] / emission.shape[1]\n",
        "    word_start = 0\n",
        "    for i in range(len(tokens)):\n",
        "        if i != 0 and tokens[i - 1] == \"|\":\n",
        "            word_start = timesteps[i]\n",
        "        if tokens[i] != \"|\":\n",
        "            plt.annotate(tokens[i].upper(), (timesteps[i] * ratio, waveform.max() * 1.02), size=14)\n",
        "        elif i != 0:\n",
        "            word_end = timesteps[i]\n",
        "            ax.axvspan(word_start * ratio, word_end * ratio, alpha=0.1, color=\"red\")\n",
        "\n",
        "    xticks = ax.get_xticks()\n",
        "    plt.xticks(xticks, xticks / SR)\n",
        "    ax.set_xlabel(\"Time\")\n",
        "    ax.set_xlim(0, waveform.shape[0])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def test_distance_algorithms(data):\n",
        "    \"\"\"\n",
        "    Test the Distances algorithms (DTW & Euclidean) as the most naive\n",
        "    implementations.\n",
        "    \"\"\"\n",
        "    x_train, y_train = data.get_data('train')\n",
        "    x_val, y_val = data.get_data('val')\n",
        "    x_test, y_test = data.get_data('test')\n",
        "\n",
        "    dtw = DTWModel(x_train, y_train)\n",
        "    dtw.add_data(x_val, y_val)\n",
        "    predictions_dtw = dtw.classify_using_DTW_distance(x_test)\n",
        "    print('Predictions:')\n",
        "    print(predictions_dtw[:5])\n",
        "    print('True labels:')\n",
        "    print(y_test[:5])\n",
        "    print('Testing DTW algorithm...')\n",
        "    wer_error = wer(y_test, predictions_dtw)\n",
        "    cer_error = cer(y_test, predictions_dtw)\n",
        "    print(f'DTW Test WER: {wer_error:.4f}')\n",
        "    print(f'DTW Test CER: {cer_error:.4f}')\n",
        "    print('DTW tested successfully')\n",
        "\n",
        "    # print('Testing Euclidean algorithm...')\n",
        "    # euclidean = EuclideanModel(x_train, y_train)\n",
        "    # euclidean.add_data(x_val, y_val)\n",
        "    # predictions_euclidean = euclidean.classify_using_euclidean_distance(x_test)\n",
        "    # wer_error = wer(y_test, predictions_euclidean)\n",
        "    # cer_error = cer(y_test, predictions_euclidean)\n",
        "    # print('Predictions:')\n",
        "    # print(predictions_euclidean[:5])\n",
        "    # print('True labels:')\n",
        "    # print(y_test[:5])\n",
        "    # print(f'Euclidean Test WER: {wer_error:.4f}')\n",
        "    # print(f'Euclidean Test CER: {cer_error:.4f}')\n",
        "    # print('Euclidean tested successfully')\n",
        "\n",
        "\n",
        "def main():\n",
        "    print('--- Start running ---')\n",
        "    # test_distance_algorithms(data)\n",
        "\n",
        "    train_data_set = AN4Dataset('train')\n",
        "    test_data_set = AN4Dataset('test')\n",
        "    train_loader = DataLoader(dataset=train_data_set,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              shuffle=True,\n",
        "                              collate_fn=lambda x: data_processing(x, 'train'))\n",
        "    test_loader = DataLoader(dataset=test_data_set,\n",
        "                             batch_size=BATCH_SIZE,\n",
        "                             shuffle=False,\n",
        "                             collate_fn=lambda x: data_processing(x, 'val'))\n",
        "\n",
        "    # lang_model = language_model.LanguageModel(vocabulary)\n",
        "    print('Training the language models...')\n",
        "    # language_model.train_all_data(lang_model, y_train+y_val)\n",
        "    print('Language models trained successfully')\n",
        "    # ctc_lstm = SpeechRecognitionModel(hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
        "    #                                             hparams['n_class'], hparams['n_feats'], hparams['stride'],\n",
        "    #                                             hparams['dropout']\n",
        "    #                                             ).to(device)\n",
        "\n",
        "    ctc_lstm = LSTMModel(0)\n",
        "    lossFunc = nn.CTCLoss(blank=28, zero_infinity=True).to(device)\n",
        "    if os.path.exists(CTC_MODEL_PATH):\n",
        "        print('Loading the models...')\n",
        "        load_model(ctc_lstm, CTC_MODEL_PATH)\n",
        "        print('Model loaded successfully')\n",
        "    else:  # Train the models\n",
        "        print('Training the models...')\n",
        "        train_all_data(ctc_lstm, train_loader, lossFunc)\n",
        "    print('Model trained successfully')\n",
        "\n",
        "    test(ctc_lstm, test_loader, lossFunc)\n",
        "\n",
        "    # Evaluate the models on the test set\n",
        "    # print('Evaluating the models...')\n",
        "    # x_test, y_test = data.get_data('train')\n",
        "    # test_wer, test_cer = evaluate(ctc_lstm, x_test, y_test)\n",
        "    # print(f'Test WER: {test_wer:.4f}')\n",
        "    # print(f'Test CER: {test_cer:.4f}')\n",
        "    # print('Model evaluated successfully')\n",
        "    print('-- Finished running ---')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vj7zzNxK-h-",
        "outputId": "c58241f1-cee4-4f07-947c-f227c1a32be9"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Start running ---\n",
            "Training the language models...\n",
            "Language models trained successfully\n",
            "Loading the models...\n",
            "Model loaded successfully\n",
            "Model trained successfully\n",
            "\n",
            "evaluating...\n",
            "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''] ['enter two nine eight one', 'repeat', 'erase u d b e five', 'rubout u b u t r six', 'enter one oh four', 'erase a b f n q fifty seven', 't r t f i seven', 'rubout c b w x v four', 'w y a t u seventy seven seventy seven', 'enter eight', 'enter seven one five four', 'no', 'g f t u one three three two', 'l k k n thirty eight', 'rubout v z j h p seven thirty six', 'rubout g m e f three nine', 'erase c q q f seven', 'b a o z five three', 'go', 'rubout n i m n one', 'no', 'enter nine one six nine', 'enter eight ninety seven', 'j p e g four', 'erase x a g n a six thirty five', 'enter twenty one', 'enter thirty', 'd a j n h ninety six', 's y f c d nine six oh one seven', 'enter one seventy six', 'enter nine two eight', 'erase e d z e fifty six', 'enter eight thirteen', 'rubout e y f c x four', 'r h n g a fifty four eighty three', 'rubout n s v h t six forty nine', 'x n k u eight', 'enter four five eight two one', 'erase p q y v t five ninety eight', 'enter two eighteen', 'enter seven', 'a b v j ninety four', 'rubout g r a g eighty five', 'enter five six eight', 'stop', 'x k i t b two six one one', 'yes', 'no', 'erase k m h n i six oh five', 'erase f o x k eight', 'w o o d', 'p a t t e r s o n', 'p o w e l l', 'h o u s e r', 's a n d l e r', 'k a u f m a n', 'y a n a s a k', 'm y e r s', 'l e v i s o n', 'g i n s b e r g', 'c i n d y', 'j a n e t', 'v a n e s s a', 'a l a n', 'd a v i d', 'e r i c', 'i v a n', 'j o h n', 'j e f f r e y', 'm i c h a e l', 'one three seven', 'one fifty', 'three zero two one', 'fifty two nineteen', 'two forty', 'sixty six thirty three', 'twelve thirty three', 'two two six', 'nineteen', 'a seven', 'm e l v i n', 's p e e r', 'w i l l e t t', 'c e n t r e', 'w i l l o w b e n d', 'b i r c h w o o d', 'm o r e w o o d', 'c e d a r v i l l e', 'p h i n n e y', 'm a r g a r e t m o r r i s o n', 'p l e a s a n t h i l l s', 'm c k e e s r o c k s', 'b r e n t w o o d', 'p i t t s b u r g h', 'r o c h e s t e r', 'p i t t s b u r g h', 'p i t t s b u r g h', 'p i t t s b u r g h', 'l e x i n g t o n', 'p i t t s b u r g h', 'one five two three six', 'one five one three six', 'one five two two seven', 'one five two three two', 'one four six one eight', 'one five two one seven', 'one five two one three', 'one five two two four', 'oh two one seven three', 'one five two one three', 'six five five eight seven four zero', 'three three one oh one eight eight', 'eight eight four three six four eight', 'six eight three six zero two seven', 'seven one six two four four six seven one four', 'four two one eight eight six oh', 'two six eight four six nine four', 'six eight three three oh seven five', 'eight six two oh three eight seven', 'four one two two six eight four one four two', 'eleven twenty seven fifty seven', 'twelve twenty nine fifty nine', 'ten twenty seven sixty two', 'may second nineteen sixty five', 'march seven nineteen sixty seven', 'five twenty seven sixty six', 'january seventh nineteen sixty seven', 'november ninth sixty five']\n",
            "['', ''] ['june eighteenth nineteen sixty eight', 'october twenty four nineteen seventy']\n",
            "-- Finished running ---\n"
          ]
        }
      ]
    }
  ]
}