{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install jiwer torch torchaudio comet_ml fastdtw"
      ],
      "metadata": {
        "id": "nUlREEZGu7XI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchaudio.models.decoder import ctc_decoder\n",
        "from torchaudio.models.decoder._ctc_decoder import download_pretrained_files\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "from jiwer import wer, cer\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from comet_ml import Experiment\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "files = download_pretrained_files(\"librispeech-4-gram\")\n"
      ],
      "metadata": {
        "id": "wjFoB3526LH-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d12257dc-c3a4-4dbf-c94c-05848df5bd5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchaudio/models/decoder/_ctc_decoder.py:62: UserWarning: The built-in flashlight integration is deprecated, and will be removed in future release. Please install flashlight-text. https://pypi.org/project/flashlight-text/ For the detail of CTC decoder migration, please see https://github.com/pytorch/audio/issues/3088.\n",
            "  warnings.warn(\n",
            "100%|██████████| 4.97M/4.97M [00:00<00:00, 64.0MB/s]\n",
            "100%|██████████| 57.0/57.0 [00:00<00:00, 88.2kB/s]\n",
            "100%|██████████| 2.91G/2.91G [00:51<00:00, 61.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "ikPqDXlNckS6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "257dc711-87c6-4b4a-bacc-11a7cef6ead4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Numerical constants\n",
        "SR = 16000\n",
        "FILE_2CHECK = 13\n",
        "HOP_LEN = 160\n",
        "N_FFT = 400\n",
        "N_MELS = 128\n",
        "N_MFCC = 13\n",
        "N_EPOCHS = 50\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 0.0001\n",
        "WEIGHT_DECAY = 0.001\n",
        "N_CNN_LAYERS = 3\n",
        "N_RNN_LAYERS = 5\n",
        "RNN_DIM = 512\n",
        "STRIDE = 2\n",
        "HIDDEN_DIM = 256\n",
        "EMBED_DIM = 300\n",
        "NUM_CLASSES = 29\n",
        "DROPOUT = 0.1\n",
        "\n",
        "# TODO - need to change the following to the current path of the data, model,\n",
        "# lexicon and tokens files.\n",
        "CTC_MODEL_PATH = ''\n",
        "DATA_PATH = ''\n",
        "\n",
        "LEXICON_PATH = ''\n",
        "TOKENS_PATH = ''\n"
      ],
      "metadata": {
        "id": "r_nRg0LDLT0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from torch.nn.functional import pairwise_distance\n",
        "from scipy.spatial.distance import cdist\n",
        "from fastdtw import fastdtw\n",
        "\n",
        "MAX_LEN = 102400\n",
        "\n",
        "\n",
        "def extract_features(wavs):\n",
        "    \"\"\"\n",
        "    Extract MFCC features from the given audios batch.\n",
        "    More ideas: try Time Domain / STFT / Mel Spectrogram\n",
        "    \"\"\"\n",
        "    mfcc_transform = torchaudio.transforms.MFCC(\n",
        "        sample_rate=SR, n_mfcc=N_MFCC,\n",
        "        melkwargs={'hop_length': HOP_LEN, 'n_fft': N_FFT, 'n_mels': N_MELS})\n",
        "    mfcc_batch = mfcc_transform(wavs).squeeze()\n",
        "    return mfcc_batch\n",
        "\n",
        "\n",
        "class DTWModel:\n",
        "    def __init__(self, x_train, y_train):\n",
        "        self.x_train = extract_features(x_train)\n",
        "        self.y_train = y_train\n",
        "\n",
        "    def classify_using_DTW_distance(self, audio_files) -> tp.List[int]:\n",
        "        \"\"\"\n",
        "        function to classify a given audio using DTW distance.\n",
        "        audio_files: list of audio file paths or a a batch of audio files\n",
        "         of shape [Batch, Channels, Time]\n",
        "        return: list of predicted label for each batch entry\n",
        "        \"\"\"\n",
        "        predictions = []\n",
        "\n",
        "        for wav in tqdm(audio_files):\n",
        "            wav = torch.cat([wav, torch.zeros((1, MAX_LEN-wav.size(1)))], dim=1)\n",
        "            best_dist, best_label = float('inf'), None\n",
        "            mfcc = extract_features(wav)\n",
        "            for i, x in enumerate(self.x_train):\n",
        "                # cur_dist = self.DTW_distance(mfcc[0], x[0])\n",
        "                cur_dist = fastdtw(mfcc, x)[0]\n",
        "                if cur_dist < best_dist:\n",
        "                    best_dist, best_label = cur_dist, self.y_train[i]\n",
        "            predictions.append(best_label)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def add_data(self, x, y):\n",
        "        wavs = []\n",
        "        for wav in x:\n",
        "            wav = torch.cat([wav, torch.zeros((1, MAX_LEN-wav.size(1)))], dim=1)\n",
        "            wavs.append(wav)\n",
        "        wavs = torch.stack(wavs)\n",
        "        self.x_train = torch.cat([self.x_train, extract_features(wavs)])\n",
        "        self.y_train = self.y_train + y\n",
        "\n",
        "    @staticmethod\n",
        "    def DTW_distance(x, y):\n",
        "        n, m = len(x), len(y)\n",
        "        dtw_mat = np.zeros((n, m))\n",
        "        dtw_mat[0, 0] = torch.sum(pairwise_distance(x[0], y[0], p=2))\n",
        "\n",
        "        for i in range(1, n):\n",
        "            dtw_mat[i, 0] = torch.sum(pairwise_distance(x[i], y[0], p=2))\\\n",
        "                            +dtw_mat[i-1, 0]\n",
        "\n",
        "        for j in range(1, m):\n",
        "            dtw_mat[0, j] = torch.sum(pairwise_distance(x[0], y[j], p=2))\\\n",
        "                            +dtw_mat[0, j-1]\n",
        "\n",
        "        for i in range(1, n):\n",
        "            for j in range(1, m):\n",
        "                cost = torch.sum(pairwise_distance(x[i], y[j], p=2))\n",
        "                dtw_mat[i, j] = cost+min(dtw_mat[i-1, j],\n",
        "                                         dtw_mat[i, j-1],\n",
        "                                         dtw_mat[i-1, j-1])\n",
        "\n",
        "        return dtw_mat[n-1, m-1]\n",
        "\n",
        "\n",
        "class EuclideanModel:\n",
        "    def __init__(self, x_train, y_train):\n",
        "        self.x_train = extract_features(x_train)\n",
        "        self.y_train = y_train\n",
        "\n",
        "    def classify_using_euclidean_distance(self, audio_files) -> tp.List[int]:\n",
        "        \"\"\"\n",
        "        function to classify a given audio using euclidean distance.\n",
        "        audio_files: list of audio file paths or a a batch of audio files\n",
        "         of shape [Batch, Channels, Time]\n",
        "        return: list of predicted label for each batch entry\n",
        "        \"\"\"\n",
        "        predictions = []\n",
        "\n",
        "        for wav in tqdm(audio_files):\n",
        "            wav = torch.cat([wav, torch.zeros((1, MAX_LEN-wav.size(1)))], dim=1)\n",
        "            mfcc = extract_features(wav)\n",
        "            best_dist, best_label = float('inf'), None\n",
        "            for i, x in enumerate(self.x_train):\n",
        "                cur_dist = torch.norm(mfcc - x)\n",
        "                if cur_dist < best_dist:\n",
        "                    best_dist, best_label = cur_dist, self.y_train[i]\n",
        "            predictions.append(best_label)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def add_data(self, x, y):\n",
        "        wavs = []\n",
        "        for wav in x:\n",
        "            wav = torch.cat([wav, torch.zeros((1, MAX_LEN-wav.size(1)))], dim=1)\n",
        "            wavs.append(wav)\n",
        "        wavs = torch.stack(wavs)\n",
        "        self.x_train = torch.cat([self.x_train, extract_features(wavs)])\n",
        "        self.y_train = self.y_train + y\n"
      ],
      "metadata": {
        "id": "dEOUWmnYkGQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Data:\n",
        "    def __init__(self):\n",
        "        print('Loading data...')\n",
        "        self.data_dir = 'an4'\n",
        "        self.x_train_paths, self.x_train, self.y_train = self.load_data('train')\n",
        "        self.x_val_paths, self.x_val, self.y_val = self.load_data('val')\n",
        "        self.x_test_paths, self.x_test, self.y_test = self.load_data('test')\n",
        "        print('Data loaded successfully')\n",
        "\n",
        "    def load_data(self, split):\n",
        "        \"\"\"\n",
        "        Load the data from the provided 'an4' folder, and split it into train, dev, and test sets.\n",
        "        \"\"\"\n",
        "        audio_dir = os.path.join(self.data_dir, split, 'an4', 'wav')\n",
        "        transcript_dir = os.path.join(self.data_dir, split, 'an4', 'txt')\n",
        "\n",
        "        audio_files = sorted(os.listdir(audio_dir))\n",
        "        transcript_files = sorted(os.listdir(transcript_dir))\n",
        "\n",
        "        audio_paths = [os.path.join(audio_dir, file) for file in audio_files]\n",
        "        transcript_paths = [os.path.join(transcript_dir, file) for file in\n",
        "                            transcript_files]\n",
        "\n",
        "        audios, transcripts = [], []\n",
        "\n",
        "        for audio_path, transcript_path in zip(audio_paths, transcript_paths):\n",
        "            with open(transcript_path, 'r') as f:\n",
        "                transcript = f.read().strip()\n",
        "                transcripts.append(transcript)\n",
        "\n",
        "        loaded_audios = [torchaudio.load(audio)[0] for audio in audio_paths]\n",
        "        return audio_paths, loaded_audios, transcripts\n",
        "\n",
        "    def get_data(self, split):\n",
        "        if split == 'train':\n",
        "            return self.x_train, self.y_train\n",
        "        elif split == 'val':\n",
        "            return self.x_val, self.y_val\n",
        "        elif split == 'test':\n",
        "            return self.x_test, self.y_test\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid data split '{split}'\")\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n",
        "        waveform = waveform.numpy()\n",
        "\n",
        "        num_channels, num_frames = waveform.shape\n",
        "        time_axis = torch.arange(0, num_frames) / sample_rate\n",
        "\n",
        "        figure, axes = plt.subplots(num_channels, 1)\n",
        "        if num_channels == 1:\n",
        "            axes = [axes]\n",
        "        for c in range(num_channels):\n",
        "            axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
        "            axes[c].grid(True)\n",
        "            if num_channels > 1:\n",
        "                axes[c].set_ylabel(f'Channel {c + 1}')\n",
        "            if xlim:\n",
        "                axes[c].set_xlim(xlim)\n",
        "            if ylim:\n",
        "                axes[c].set_ylim(ylim)\n",
        "        figure.suptitle(title)\n",
        "        plt.show(block=False)\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_mel_spec(waveform):\n",
        "        mel_specgram = torchaudio.transforms.MelSpectrogram(SR, n_mels=N_MELS)(waveform)\n",
        "        return mel_specgram\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_mfcc(mfcc, title=\"MFCC\", xlim=None, ylim=None):\n",
        "        fig, ax = plt.subplots()\n",
        "        im = ax.imshow(mfcc, origin='lower', aspect='auto')\n",
        "        fig.colorbar(im, ax=ax)\n",
        "        ax.set(title=title, xlabel='Time', ylabel='MFCC')\n",
        "        if xlim:\n",
        "            ax.set_xlim(xlim)\n",
        "        if ylim:\n",
        "            ax.set_ylim(ylim)\n",
        "        plt.show(block=False)\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_mel_spec(mel_spec, title=\"Mel Spectrogram\", xlim=None, ylim=None):\n",
        "        mel_spec = mel_spec.squeeze(0).numpy()\n",
        "        fig, ax = plt.subplots()\n",
        "        im = ax.imshow(mel_spec, origin='lower', aspect='auto')\n",
        "        fig.colorbar(im, ax=ax)\n",
        "        ax.set(title=title, xlabel='Time', ylabel='Frequency (Hz)')\n",
        "        if xlim:\n",
        "            ax.set_xlim(xlim)\n",
        "        if ylim:\n",
        "            ax.set_ylim(ylim)\n",
        "        plt.show(block=False)\n",
        "\n",
        "\n",
        "def test_distance_algorithms(data):\n",
        "    \"\"\"\n",
        "    Test the Distances algorithms (DTW & Euclidean) as the most naive\n",
        "    implementations.\n",
        "    \"\"\"\n",
        "    x_train, y_train = data.get_data('train')\n",
        "    x_val, y_val = data.get_data('val')\n",
        "    x_test, y_test = data.get_data('test')\n",
        "\n",
        "    dtw = DTWModeזl(x_train, y_train)\n",
        "    dtw.add_data(x_val, y_val)\n",
        "    predictions_dtw = dtw.classify_using_DTW_distance(x_test)\n",
        "    print('Predictions:')\n",
        "    print(predictions_dtw[:5])\n",
        "    print('True labels:')\n",
        "    print(y_test[:5])\n",
        "    print('Testing DTW algorithm...')\n",
        "    wer_error = wer(y_test, predictions_dtw)\n",
        "    cer_error = cer(y_test, predictions_dtw)\n",
        "    print(f'DTW Test WER: {wer_error:.4f}')\n",
        "    print(f'DTW Test CER: {cer_error:.4f}')\n",
        "    print('DTW tested successfully')\n"
      ],
      "metadata": {
        "id": "SxmtCmUegorM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoders"
      ],
      "metadata": {
        "id": "3SV88qfigz8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoders\n",
        "def greedy_decoder(output, labels, label_lengths,\n",
        "                   blank_label=28, collapse_repeated=True):\n",
        "    arg_maxes = torch.argmax(output, dim=2)\n",
        "    decodes = []\n",
        "    targets = []\n",
        "    text_transform = TextTransform()\n",
        "    for i, args in enumerate(arg_maxes):\n",
        "        decode = []\n",
        "        target = text_transform.int_to_text(labels[i][:label_lengths[i]].tolist())\n",
        "        targets.append(target)\n",
        "        for j, index in enumerate(args):\n",
        "            if index != blank_label:\n",
        "                if collapse_repeated and j != 0 and index == args[j -1]:\n",
        "                    continue\n",
        "                decode.append(index.item())\n",
        "        pred = text_transform.int_to_text(decode)\n",
        "        decodes.append(pred)\n",
        "\n",
        "    return decodes, targets\n",
        "\n",
        "\n",
        "def beam_decoder(output, labels, label_lengths, compare=True):\n",
        "    output = output.to('cpu')\n",
        "    beam_decoder = ctc_decoder(lexicon=LEXICON_PATH, tokens=TOKENS_PATH, lm=files.lm, blank_token='|',\n",
        "                               sil_token='SPACE', lm_weight=1, beam_size=100,\n",
        "                               word_score=-1)\n",
        "    beam_search_result = beam_decoder(output.contiguous())\n",
        "    text_transform = TextTransform()\n",
        "    preds, actuals = [], []\n",
        "    for i in range(output.shape[0]):\n",
        "        pred = \" \".join(beam_search_result[i][0].words).strip()\n",
        "        preds.append(pred)\n",
        "\n",
        "        if compare:\n",
        "          actual = text_transform.int_to_text(labels[i][:label_lengths[i]].tolist())\n",
        "          actuals.append(actual)\n",
        "\n",
        "    return preds, actuals\n",
        "\n"
      ],
      "metadata": {
        "id": "UNpfsbTt6Oi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data processing"
      ],
      "metadata": {
        "id": "hRKPCpC8gyJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextTransform:\n",
        "    def __init__(self):\n",
        "        char_map_str = \"\"\"\n",
        "            ' 0\n",
        "            SPACE 1\n",
        "            a 2\n",
        "            b 3\n",
        "            c 4\n",
        "            d 5\n",
        "            e 6\n",
        "            f 7\n",
        "            g 8\n",
        "            h 9\n",
        "            i 10\n",
        "            j 11\n",
        "            k 12\n",
        "            l 13\n",
        "            m 14\n",
        "            n 15\n",
        "            o 16\n",
        "            p 17\n",
        "            q 18\n",
        "            r 19\n",
        "            s 20\n",
        "            t 21\n",
        "            u 22\n",
        "            v 23\n",
        "            w 24\n",
        "            x 25\n",
        "            y 26\n",
        "            z 27\n",
        "            \"\"\"\n",
        "        self.char_map = {}\n",
        "        self.index_map = {}\n",
        "        for line in char_map_str.strip().split('\\n'):\n",
        "            ch, index = line.split()\n",
        "            self.char_map[ch] = int(index)\n",
        "            self.index_map[int(index)] = ch\n",
        "        self.index_map[1] = ' '\n",
        "\n",
        "    def text_to_int(self, text):\n",
        "        int_sequence = []\n",
        "        for c in text:\n",
        "            if c == ' ':\n",
        "                ch = self.char_map['SPACE']\n",
        "            else:\n",
        "                ch = self.char_map[c]\n",
        "            int_sequence.append(ch)\n",
        "        return int_sequence\n",
        "\n",
        "    def int_to_text(self, labels):\n",
        "        string = []\n",
        "        for i in labels:\n",
        "            string.append(self.index_map[i])\n",
        "        return ''.join(string).replace('SPACE', ' ')\n"
      ],
      "metadata": {
        "id": "Lo_obzPXLb8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2ep0TP9KzRH"
      },
      "outputs": [],
      "source": [
        "# data loader\n",
        "class AN4Dataset(Dataset):\n",
        "    def __init__(self, split, transform=None, target_transform=None):\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "        audio_dir = os.path.join(DATA_PATH, split, 'an4', 'wav')\n",
        "        transcript_dir = os.path.join(DATA_PATH, split, 'an4', 'txt')\n",
        "\n",
        "        audio_files = sorted(os.listdir(audio_dir))\n",
        "        transcript_files = sorted(os.listdir(transcript_dir))\n",
        "\n",
        "        audio_paths = [os.path.join(audio_dir, file) for file in audio_files if file.endswith('wav')]\n",
        "        transcript_paths = [os.path.join(transcript_dir, file) for file in\n",
        "                            transcript_files if file.endswith('txt')]\n",
        "\n",
        "        self.audios, self.transcripts = [], []\n",
        "\n",
        "        for audio_path, transcript_path in zip(audio_paths, transcript_paths):\n",
        "            with open(transcript_path, 'r') as f:\n",
        "                transcript = f.read().strip()\n",
        "                self.transcripts.append(transcript)\n",
        "\n",
        "        self.loaded_audios = [torchaudio.load(audio)[0] for audio in audio_paths]\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.loaded_audios)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.loaded_audios[idx], self.transcripts[idx]\n",
        "\n",
        "\n",
        "def data_processing(data, data_type=\"train\"):\n",
        "    inputs, inputs_lengths, labels, labels_length = [], [], [], []\n",
        "\n",
        "    if data_type == \"train\":\n",
        "        transform = nn.Sequential(\n",
        "            torchaudio.transforms.MelSpectrogram(sample_rate=SR, n_mels=N_MELS),\n",
        "            torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
        "            torchaudio.transforms.TimeMasking(time_mask_param=100))\n",
        "    else:\n",
        "        transform = torchaudio.transforms.MelSpectrogram(sample_rate=SR, n_mels=N_MELS)\n",
        "\n",
        "    text_transform = TextTransform()\n",
        "    for (wav, transcript) in data:\n",
        "        spec = transform(wav).squeeze(0).transpose(0, 1)\n",
        "        inputs.append(spec)\n",
        "        inputs_lengths.append(spec.shape[0] // 2)\n",
        "        label = torch.Tensor(text_transform.text_to_int(str(transcript).lower()))\n",
        "        labels.append(label)\n",
        "        labels_length.append(len(label))\n",
        "\n",
        "    max_length = (max(inputs_lengths) * 2) + 1\n",
        "\n",
        "    # Pad tensors and create the big tensor\n",
        "    spectrograms = torch.zeros((len(inputs), max_length, N_MELS))\n",
        "    for i, tensor in enumerate(inputs):\n",
        "        spectrograms[i, :tensor.shape[0], :] = tensor[:, :]\n",
        "\n",
        "    spectrograms = spectrograms.unsqueeze(1).transpose(2, 3)\n",
        "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "    return spectrograms, labels, inputs_lengths, labels_length\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Models"
      ],
      "metadata": {
        "id": "OOci_zcSgvXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicLSTMModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(BasicLSTMModel, self).__init__()\n",
        "        self.rnn = nn.LSTM(input_size=N_MELS, hidden_size=HIDDEN_DIM,\n",
        "                           bidirectional=True, batch_first=True)\n",
        "\n",
        "        self.fc = nn.Linear(HIDDEN_DIM*2, NUM_CLASSES)\n",
        "\n",
        "    def forward(self, x):\n",
        "        sizes = x.size()\n",
        "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
        "        x = x.transpose(1, 2)  # (batch, time, feature)\n",
        "        rnn_output, _ = self.rnn(x)\n",
        "        output = self.fc(rnn_output)\n",
        "\n",
        "        return output\n",
        "\n"
      ],
      "metadata": {
        "id": "mmBxwBRB5ZBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdvanceLSTMModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AdvanceLSTMModel, self).__init__()\n",
        "\n",
        "        # RNN layers\n",
        "        self.rnn = nn.LSTM(input_size=512, hidden_size=128,\n",
        "                           num_layers=3, batch_first=True, bidirectional=True)\n",
        "\n",
        "        self.layer_norm1 = nn.LayerNorm(512)\n",
        "        self.layer_norm2 = nn.LayerNorm(256)\n",
        "        self.layer_norm3 = nn.LayerNorm(128)\n",
        "        self.layer_norm4 = nn.LayerNorm(5504)\n",
        "        self.dropout = nn.Dropout(0.15)\n",
        "\n",
        "        self.fc1 = nn.Linear(5504, 512)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3)\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 29)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        sizes = x.size()\n",
        "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
        "        x = x.transpose(1, 2)  # (batch, time, feature)\n",
        "        x = self.layer_norm4(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer_norm1(x)\n",
        "        x, _ = self.rnn(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer_norm2(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer_norm3(x)\n",
        "        output = self.fc3(x)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "6Pgyx-hLigqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNLayerNorm(nn.Module):\n",
        "    def __init__(self, n_feats):\n",
        "        super(CNNLayerNorm, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(2, 3).contiguous()\n",
        "        x = self.layer_norm(x)\n",
        "        return x.transpose(2, 3).contiguous()\n",
        "\n",
        "\n",
        "class ResidualCNN(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel, stride, dropout,\n",
        "                 n_feats):\n",
        "        super(ResidualCNN, self).__init__()\n",
        "\n",
        "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride,\n",
        "                              padding=kernel // 2)\n",
        "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride,\n",
        "                              padding=kernel // 2)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
        "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = self.layer_norm1(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.cnn1(x)\n",
        "        x = self.layer_norm2(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.cnn2(x)\n",
        "        x += residual\n",
        "        return x\n",
        "\n",
        "class BidirectionalGRU(nn.Module):\n",
        "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
        "        super(BidirectionalGRU, self).__init__()\n",
        "\n",
        "        self.BiGRU = nn.GRU(\n",
        "            input_size=rnn_dim, hidden_size=hidden_size,\n",
        "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
        "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        x = F.gelu(x)\n",
        "        x, _ = self.BiGRU(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FinalASRModel(nn.Module):\n",
        "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class,\n",
        "                 n_feats, stride, dropout):\n",
        "        super(FinalASRModel, self).__init__()\n",
        "        n_feats = n_feats // 2\n",
        "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3 // 2)\n",
        "        self.rescnn_layers = nn.Sequential(*[\n",
        "            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout,\n",
        "                        n_feats=n_feats)\n",
        "            for _ in range(n_cnn_layers)\n",
        "        ])\n",
        "        self.fully_connected = nn.Linear(n_feats * 32, rnn_dim)\n",
        "        self.birnn_layers = nn.Sequential(*[\n",
        "            BidirectionalGRU(rnn_dim=rnn_dim if i == 0 else rnn_dim * 2,\n",
        "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i == 0)\n",
        "            for i in range(n_rnn_layers)\n",
        "        ])\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(rnn_dim * 2, rnn_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(rnn_dim, n_class)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x)\n",
        "        x = self.rescnn_layers(x)\n",
        "        sizes = x.size()\n",
        "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])\n",
        "         # (batch, feature, time)\n",
        "        x = x.transpose(1, 2)  # (batch, time, feature)\n",
        "        x = self.fully_connected(x)\n",
        "        x = self.birnn_layers(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "ceCaO2YleiRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and test loops"
      ],
      "metadata": {
        "id": "PiTrhIVWyLYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, path):\n",
        "    \"\"\"\n",
        "    Saves a pytorch models to the given path.\n",
        "    \"\"\"\n",
        "    torch.save(model.state_dict(), '{}'.format(path))\n",
        "\n",
        "\n",
        "def load_model(model, path):\n",
        "    \"\"\"\n",
        "    Loads a pytorch models from the given path. The models should already by\n",
        "    created (e.g. by calling the constructor) and should be passed as an argument.\n",
        "    \"\"\"\n",
        "\n",
        "    model.load_state_dict(torch.load('{}'.format(path)))\n",
        "\n",
        "\n",
        "def test(model, test_loader, criterion, experiment, counter=0, isEval=False,\n",
        "         decoder=greedy_decoder):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    t_loss, test_cer, test_wer = [], [], []\n",
        "    with experiment.test():\n",
        "        with torch.no_grad():\n",
        "            for i, _data in enumerate(test_loader):\n",
        "                spectrograms, labels, input_lengths, label_lengths = _data\n",
        "                spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "                output = model(spectrograms)  # (batch, time, n_class)\n",
        "                output = F.log_softmax(output, dim=2)\n",
        "                output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "                loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "                t_loss.append(loss.item())\n",
        "\n",
        "                decoded_preds, decoded_targets = decoder(output.transpose(0, 1),\n",
        "                                                         labels, label_lengths)\n",
        "                for j in range(len(decoded_preds)):\n",
        "                    test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
        "                    test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
        "\n",
        "        avg_cer = sum(test_cer)/len(test_cer)\n",
        "        avg_wer = sum(test_wer)/len(test_wer)\n",
        "        avg_loss = sum(t_loss)/len(t_loss)\n",
        "\n",
        "        if isEval:\n",
        "          experiment.log_metric('test_loss', avg_loss, step=counter)\n",
        "          experiment.log_metric('cer', avg_cer, step=counter)\n",
        "          experiment.log_metric('wer', avg_wer, step=counter)\n",
        "        else:\n",
        "          print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(avg_loss, avg_cer, avg_wer))\n",
        "\n",
        "\n",
        "def train(model, train_loader, val_loader, criterion, experiment):\n",
        "    data_len = len(train_loader.dataset)\n",
        "    model.train()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), LEARNING_RATE)\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=LEARNING_RATE,\n",
        "                                            steps_per_epoch=int(len(train_loader)),\n",
        "                                            epochs=N_EPOCHS,\n",
        "                                            anneal_strategy='linear')\n",
        "    with experiment.train():\n",
        "      counter = 0\n",
        "      for epoch in range(N_EPOCHS):\n",
        "          model.train()\n",
        "          if (epoch + 1) % 10 == 0:\n",
        "              save_model(model, CTC_MODEL_PATH)\n",
        "\n",
        "          e_loss = 0\n",
        "          losses = []\n",
        "          for batch_idx, _data in enumerate(train_loader):\n",
        "              spectrograms, labels, input_lengths, label_lengths = _data\n",
        "              spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "              optimizer.zero_grad()\n",
        "              output = model(spectrograms)  # (batch, time, n_class)\n",
        "              output = F.log_softmax(output, dim=2)\n",
        "              output = output.transpose(0, 1)  # (time, batch, n_class)\n",
        "\n",
        "              loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "              loss.backward()\n",
        "\n",
        "              # log it\n",
        "              optimizer.step()\n",
        "              scheduler.step()\n",
        "              e_loss += loss.item()\n",
        "              losses.append(loss.item())\n",
        "\n",
        "          print(f\"Train Epoch: {epoch}, loss = {e_loss}\")\n",
        "          experiment.log_metric('loss', sum(losses)/len(losses), step=epoch+1)\n",
        "          experiment.log_metric('learning_rate', scheduler.get_last_lr(),\n",
        "                                step=epoch+1)\n",
        "\n",
        "          test(model, val_loader, criterion, experiment, epoch+1, True)\n",
        "\n",
        "    save_model(model, CTC_MODEL_PATH)\n"
      ],
      "metadata": {
        "id": "8olxeA0FK4SA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Telemtry init"
      ],
      "metadata": {
        "id": "qd3L6fpvyPSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setp up logging platform\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "comet_api_key = \"\" # todo - add\n",
        "project_name = \"67455 Introduction to Speech Processing - Final project\"\n",
        "experiment_name = \"\" # todo - add\n",
        "\n",
        "if comet_api_key:\n",
        "  experiment = Experiment(api_key=comet_api_key, project_name=project_name, parse_args=False)\n",
        "  experiment.set_name(experiment_name)\n",
        "  experiment.display()\n",
        "else:\n",
        "  experiment = Experiment(api_key='dummy_key', disabled=True)"
      ],
      "metadata": {
        "id": "8iptwPXo2QAg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbbb103e-04df-4f2a-f435-b35c16d7c8e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "plotting"
      ],
      "metadata": {
        "id": "hmN-XsehyT-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_for_plot(dataset, model, num):\n",
        "    transform = torchaudio.transforms.MelSpectrogram(sample_rate=SR, n_mels=N_MELS)\n",
        "    for i in range(num):\n",
        "        sample, target = dataset.__getitem__(i)\n",
        "        spec = transform(sample).squeeze(0).transpose(0, 1)\n",
        "        spectrograms = torch.stack([spec], dim=0)\n",
        "        spectrograms = spectrograms.unsqueeze(1).transpose(2, 3)\n",
        "\n",
        "        model.eval()\n",
        "        spectrograms = spectrograms.to(device)\n",
        "        output = model(spectrograms)\n",
        "        output = output.to('cpu')\n",
        "        beam_decoder = ctc_decoder(lexicon=LEXICON_PATH, tokens=TOKENS_PATH,\n",
        "                                   lm=files.lm, blank_token='|',\n",
        "                                  sil_token='SPACE')\n",
        "        beam_search_result = beam_decoder(output.contiguous())\n",
        "        pred = \" \".join(beam_search_result[0][0].words).strip()\n",
        "        tokens = [c for c in pred]\n",
        "        tokens.insert(0, ' ')\n",
        "        tokens.append(' ')\n",
        "\n",
        "        plot_alignments(sample[0], output, tokens,\n",
        "                        beam_search_result[0][0].timesteps, SR)\n",
        "\n",
        "def plot_alignments(waveform, emission, tokens, timesteps, sample_rate):\n",
        "\n",
        "    t = torch.arange(waveform.size(0)) / sample_rate\n",
        "    ratio = waveform.size(0) / emission.size(1) / sample_rate\n",
        "\n",
        "    chars = []\n",
        "    words = []\n",
        "    word_start = None\n",
        "    for token, timestep in zip(tokens, timesteps * ratio):\n",
        "        if token == \" \":\n",
        "            if word_start is not None:\n",
        "                words.append((word_start, timestep))\n",
        "            word_start = None\n",
        "        else:\n",
        "            chars.append((token, timestep))\n",
        "            if word_start is None:\n",
        "                word_start = timestep\n",
        "\n",
        "    fig, axes = plt.subplots(1, 1)\n",
        "\n",
        "    def _plot(ax, xlim):\n",
        "        ax.plot(t, waveform)\n",
        "        for token, timestep in chars:\n",
        "            ax.annotate(token.upper(), (timestep, 0.4))\n",
        "        for word_start, word_end in words:\n",
        "            ax.axvspan(word_start, word_end, alpha=0.1, color=\"red\")\n",
        "        ax.set_ylim(-0.6, 0.7)\n",
        "        ax.set_yticks([0])\n",
        "        ax.grid(True, axis=\"y\")\n",
        "\n",
        "    _plot(axes, (0.0, (timesteps * ratio)[-1]+ 3/2))\n",
        "    axes.set_xlabel(\"Time (Sec)\")\n",
        "    fig.show()\n"
      ],
      "metadata": {
        "id": "LXrcbb72yVB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "driver"
      ],
      "metadata": {
        "id": "f2Vv25S7yWe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print('--- Start running ---')\n",
        "    train_data_set = AN4Dataset('train')\n",
        "    val_data_set = AN4Dataset('val')\n",
        "    test_data_set = AN4Dataset('test')\n",
        "    train_loader = DataLoader(dataset=train_data_set,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              shuffle=True,\n",
        "                              collate_fn=lambda x: data_processing(x, 'train'))\n",
        "    val_loader = DataLoader(dataset=val_data_set,\n",
        "                             batch_size=BATCH_SIZE,\n",
        "                             shuffle=False,\n",
        "                             collate_fn=lambda x: data_processing(x, 'val'))\n",
        "    test_loader = DataLoader(dataset=test_data_set,\n",
        "                             batch_size=BATCH_SIZE,\n",
        "                             shuffle=False,\n",
        "                             collate_fn=lambda x: data_processing(x, 'test'))\n",
        "\n",
        "    ctc_lstm = FinalASRModel(N_CNN_LAYERS, N_RNN_LAYERS, RNN_DIM, NUM_CLASSES,\n",
        "                             N_MELS, STRIDE,DROPOUT).to(device)\n",
        "\n",
        "    lossFunc = nn.CTCLoss(blank=28, zero_infinity=True).to(device)\n",
        "\n",
        "    # TODO - load model using the follwoing weight file in\n",
        "    # \"https://drive.google.com/file/d/1-914Naz8MyxPLNnzywZ0_WrG7tCnIRzv/view?usp=sharing\"\n",
        "    # note to change macros to the current locations.\n",
        "\n",
        "    # load_model(ctc_lstm, CTC_MODEL_PATH)\n",
        "\n",
        "    # train(ctc_lstm, train_loader, val_loader, lossFunc, experiment)\n",
        "    # test(ctc_lstm, test_loader, lossFunc, experiment, decoder=beam_decoder)\n",
        "\n",
        "    # plot aligments from test\n",
        "    # parse_for_plot(test_data_set, ctc_lstm, 10)\n",
        "    print('-- Finished running ---')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "-vj7zzNxK-h-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "409f59fe-84f3-4df0-d912-d68139ac42b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Start running ---\n",
            "-- Finished running ---\n"
          ]
        }
      ]
    }
  ]
}